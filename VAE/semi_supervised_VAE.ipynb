{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuf9vhqE1ASe2fEz3OthoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/drug_design_JAK/blob/main/VAE/semi_supervised_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/wohlert/semi-supervised-pytorch/tree/master\n",
        "\n",
        "Semi-Supervised Learning with Deep Generative Models\n",
        "\n",
        "https://arxiv.org/abs/1406.5298\n"
      ],
      "metadata": {
        "id": "MPFqlILelhCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oCB4EoJFlbOG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "cuda = torch.cuda.is_available()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sys\n",
        "# sys.path.append(\"../../semi-supervised\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import init\n",
        "\n",
        "# from layers import GaussianSample, GaussianMerge, GumbelSoftmax\n",
        "# from inference import log_gaussian, log_standard_gaussian"
      ],
      "metadata": {
        "id": "IgB12cIdl54i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\log var = \\log \\sigma ^2$"
      ],
      "metadata": {
        "id": "OAarOs6A9hMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/wohlert/semi-supervised-pytorch/blob/master/semi-supervised/models/dgm.py\n"
      ],
      "metadata": {
        "id": "RH_w7mXDmTd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stochastic(nn.Module):\n",
        "    \"\"\"\n",
        "    Base stochastic layer that uses the reparametrization trick to\n",
        "    draw a sample from a distribution parametrized by mu and logvar\n",
        "    \"\"\"\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar) # e^(1/2 * logùúé2) = ùúé\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "class GaussianSample(Stochastic):\n",
        "    \"\"\"\n",
        "    Layer that represents a sample from a Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(GaussianSample, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        # print(f'in_dim:{in_dim}, out_dim:{out_dim}')\n",
        "        self.mu_layer = nn.Linear(in_dim, out_dim)\n",
        "        self.logvar_layer = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        mu = self.mu_layer(x)\n",
        "        # https://pytorch.org/docs/stable/_images/Softplus.png\n",
        "        logvar = F.softplus(self.logvar_layer(x))\n",
        "        return self.reparametrize(mu, logvar), mu, logvar"
      ],
      "metadata": {
        "id": "2A5MOfbb88uB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dims, sample_layer=GaussianSample):\n",
        "        \"\"\"\n",
        "        Inference network:\n",
        "        Attempts to infer the probability distribution p(z|x)\n",
        "        from the data by fitting a variational distribution q_phi(z|x).\n",
        "        (\\mu, log \\sigma ^2 ) =  q_phi(z|x)\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "            given by the number of neurons on the form\n",
        "            [input_dim, [hidden_dims], latent_dim]\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        [x_dim, h_dim, z_dim] = dims\n",
        "        neurons = [x_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(\n",
        "            neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "        self.sample = sample_layer(h_dim[-1], z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.sample(x)\n",
        "        # GaussianSample: return self.reparametrize(mu, logvar), mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Generative network:\n",
        "        Generate samples from the original distribution p(x)\n",
        "        by transforming a latent representation p_\\theta(x|z)\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "            given by the number of neurons on the form\n",
        "            [latent_dim, [hidden_dims], input_dim]\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        [z_dim, h_dim, x_dim] = dims\n",
        "        neurons = [z_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(\n",
        "            neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "        self.reconstruction = nn.Linear(h_dim[-1], x_dim)\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z):\n",
        "        for layer in self.hidden:\n",
        "            z = F.relu(layer(z))\n",
        "            z = self.reconstruction(z)\n",
        "            return self.output_activation(z)"
      ],
      "metadata": {
        "id": "x6EkNpfvmSJ9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(VAE, self).__init__()\n",
        "        \"\"\"\n",
        "        :param dims: [x_dim, z_dim, h_dim]\n",
        "        \"\"\"\n",
        "        [x_dim, z_dim, h_dim] = dims\n",
        "        # print(f\"x_dim: {x_dim}, z_dim: {z_dim}, h_dim: {h_dim}\")\n",
        "        self.z_dim = z_dim\n",
        "        self.flow = None\n",
        "        self.encoder = Encoder([x_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim, list(reversed(h_dim)), x_dim])\n",
        "        # self.kl_divergence = 0\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "    def add_flow(self, flow):\n",
        "        self.flow = flow\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :return reconstructed input\n",
        "        \"\"\"\n",
        "        z, z_mu, z_logvar = self.encoder(x)\n",
        "        return self.decoder(z)\n",
        "    def sample(self, z):\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "Fn12UBX6Bn-b"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Single hidden layer classifier with softmax output\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "        [x_dim, h_dim, y_dim] = dims\n",
        "        self.dense = nn.Linear(x_dim, h_dim)\n",
        "        self.logits = nn.Linear(h_dim, y_dim)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense(x))\n",
        "        x = F.softmax(self.logits(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "class DeepGenerativeModel(VAE):\n",
        "    \"\"\"\n",
        "    M2 code replication\n",
        "    \"\"\"\n",
        "    def __init__(self, dims):\n",
        "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
        "        # print(x_dim, self.y_dim, z_dim, h_dim)\n",
        "        super(DeepGenerativeModel, self).__init__([x_dim, z_dim, h_dim])\n",
        "        self.encoder = Encoder([x_dim+self.y_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim+self.y_dim, list(reversed(h_dim)), x_dim])\n",
        "\n",
        "        # Only used one layer h_dim[0] since classifier is defined this way\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None: m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        z, z_mu, z_logvar = self.encoder(torch.cat([x, y], dim=1))\n",
        "        x = self.decoder(torch.cat([z,y], dim=1))\n",
        "        return x\n",
        "\n",
        "    def set_classifier(self, classifier):\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def classify(self, x): return self.classifier(x)\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        \"\"\"\n",
        "        Samples from the decoder to generate x\n",
        "        :param z: latent normal variable\n",
        "        :param y: label (one-hot encoded)\n",
        "        :return: x\n",
        "        \"\"\"\n",
        "        y = y.float()\n",
        "        x = self.decoder(torch.cat([z,y], dim=1))\n",
        "        return x"
      ],
      "metadata": {
        "id": "-8kf8w3iD3L3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StackedDGM(DeepGenerativeModel):\n",
        "    \"\"\"\n",
        "    M1 + M2 from paper 'Semi-Supervised Learning with Deep Generative Models'\n",
        "    (Kingma 2014) in PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, dims, features):\n",
        "        \"\"\"\n",
        "        :param dims: dimensions of x, y, z and hidden layers\n",
        "        :param features: M1 model of VAE\n",
        "        \"\"\"\n",
        "        [x_dim, y_dim, z_dim, h_dim] = dims\n",
        "        super(StackedDGM, self).__init__([features.z_dim, y_dim, z_dim, h_dim])\n",
        "        in_dim = self.decoder.reconstruction.in_features\n",
        "        self.decoder.reconstruction = nn.Linear(in_dim, x_dim)\n",
        "\n",
        "        self.features = features\n",
        "        self.features.train(False) # Do not train features p_\\theta(x)\n",
        "\n",
        "        for param in self.features.parameters(): param.requires_grad=False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        z_sample, _, _ = self.features.encoder(x)\n",
        "        return super(StackedDGM, self).forward(z_sample, y)\n",
        "\n",
        "    def classify(self, x):\n",
        "        _, z_mu, _ = self.features.encoder(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "MME22Vy6Gfqh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dim = 10\n",
        "z_dim = 20\n",
        "h_dim = [256, 128]\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k_CWyPnDe8e",
        "outputId": "73804a19-7a4a-441f-c3d2-89fc42b435a4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_dim: 784, z_dim: 20, h_dim: [256, 128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-577a4b3b58ca>:17: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  init.xavier_normal(m.weight.data)\n",
            "<ipython-input-49-107defe77478>:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  init.xavier_normal(m.weight.data)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepGenerativeModel(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=794, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    )\n",
              "    (sample): GaussianSample(\n",
              "      (mu_layer): Linear(in_features=128, out_features=20, bias=True)\n",
              "      (logvar_layer): Linear(in_features=128, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=30, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (reconstruction): Linear(in_features=256, out_features=784, bias=True)\n",
              "    (output_activation): Sigmoid()\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (dense): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (logits): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "from urllib import request\n",
        "from torch.utils.data import Dataset\n",
        "def onehot(k):\n",
        "    \"\"\"\n",
        "    Converts a number to its one-hot or 1-of-k representation\n",
        "    vector.\n",
        "    :param k: (int) length of vector\n",
        "    :return: onehot function\n",
        "    \"\"\"\n",
        "    def encode(label):\n",
        "        y = torch.zeros(k)\n",
        "        if label < k:\n",
        "            y[label] = 1\n",
        "        return y\n",
        "    return encode\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n",
        "\n",
        "def get_mnist(location='./', batch_size=64, labels_per_class=100):\n",
        "    from functools import reduce\n",
        "    from operator import __or__\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\n",
        "    from torchvision.datasets import MNIST\n",
        "    import torchvision.transforms as transforms\n",
        "    n_labels=10\n",
        "\n",
        "    mnist_train = MNIST(location, train=True, download=True,\n",
        "                        transform=flatten_bernoulli,\n",
        "                        target_transform=onehot(n_labels))\n",
        "    mnist_valid = MNIST(location, train=False, download=True,\n",
        "                        transform=flatten_bernoulli,\n",
        "                        target_transform=onehot(n_labels))\n",
        "    def get_sampler(labels, n=None):\n",
        "        (indices, ) = np.where(reduce(__or__,\n",
        "         [labels==i for i in np.arange(n_labels)]))\n",
        "        np.random.shuffle(indices)\n",
        "        indices = np.hstack([list(filter(lambda idx: labels[idx] == i,\n",
        "                              indices))[:n] for i in range(n_labels)])\n",
        "        indices = torch.from_numpy(indices)\n",
        "        sampler = SubsetRandomSampler(indices)\n",
        "        return sampler\n",
        "    labelled = torch.utils.data.DataLoader(\n",
        "        mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n",
        "        sampler=get_sampler(mnist_train.train_labels.numpy(), labels_per_class))\n",
        "    unlabelled = torch.utils.data.DataLoader(\n",
        "        mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n",
        "        sampler=get_sampler(mnist_train.train_labels.numpy()))\n",
        "    validation = torch.utils.data.DataLoader(\n",
        "        mnist_valid, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n",
        "        sampler=get_sampler(mnist_valid.test_labels.numpy()))\n",
        "    return labelled, unlabelled, validation\n",
        ""
      ],
      "metadata": {
        "id": "bJon4slTHumS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=64, labels_per_class=10)"
      ],
      "metadata": {
        "id": "EUJHGWhvHvNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labelled), len(unlabelled), len(validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87iy3obcH4WS",
        "outputId": "d5d40c30-e713-4c89-fcdf-fd7910c342d4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 938, 157)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "mnist_train = MNIST('./', train=True, download=True,\n",
        "                        transform=flatten_bernoulli,\n",
        "                        target_transform=onehot(10))"
      ],
      "metadata": {
        "id": "bNJq3PswIBNx"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mnist_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r88ZacumPnr_",
        "outputId": "b095c019-3c54-4452-aae5-0d0ff04cb320"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je1TUpWhPy6S",
        "outputId": "b294f809-ae43-4cd1-e258-bfa13b6c212c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7OG4DWP5P4D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}