{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbzRsKwgiuNZh1QW5HDInk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "315fc6b2b6ef49f1b3623954c557c8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7ec6810671d4376bd0506a46480d1c6",
              "IPY_MODEL_ef68a98b12004437802f3e40434a8758"
            ],
            "layout": "IPY_MODEL_27fda19c7a74412987ed09935fd64f18"
          }
        },
        "e7ec6810671d4376bd0506a46480d1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffb608c9818e4294a1ae622e31791239",
            "placeholder": "​",
            "style": "IPY_MODEL_4f1a88516b464375a5dfe1a36868bb9e",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "ef68a98b12004437802f3e40434a8758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_370488654a9e496c8ec06121e9e54aa6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0f3d7252b2147a284b7698b3d7c166c",
            "value": 1
          }
        },
        "27fda19c7a74412987ed09935fd64f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffb608c9818e4294a1ae622e31791239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f1a88516b464375a5dfe1a36868bb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "370488654a9e496c8ec06121e9e54aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0f3d7252b2147a284b7698b3d7c166c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ec40334785149f6950b9137a24c2121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c39e04ca4174939b9119f992452103f",
              "IPY_MODEL_8748a12eb42446c4a2c4186c51aa9d22"
            ],
            "layout": "IPY_MODEL_007f7db9c4ec469182d950815d0c00d8"
          }
        },
        "0c39e04ca4174939b9119f992452103f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba901ff31ae8464c93f13a04ca8f4d23",
            "placeholder": "​",
            "style": "IPY_MODEL_4610d571da374ae396eacacecc783733",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "8748a12eb42446c4a2c4186c51aa9d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d06ebf8bdc3422b89d52fffd1527218",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f005e24b67f4d8f945d1675c622901c",
            "value": 1
          }
        },
        "007f7db9c4ec469182d950815d0c00d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba901ff31ae8464c93f13a04ca8f4d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4610d571da374ae396eacacecc783733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d06ebf8bdc3422b89d52fffd1527218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f005e24b67f4d8f945d1675c622901c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08bacd2d38f6427891cf01217e42cb17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb0cc63dcd384c62a199faf82196ce32",
              "IPY_MODEL_f0429fea9a07473a931e7605f6308826"
            ],
            "layout": "IPY_MODEL_e223bcbb6f5e4c7d95ca5c69512a2913"
          }
        },
        "cb0cc63dcd384c62a199faf82196ce32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb9cca19f0e242eba0240104347eb433",
            "placeholder": "​",
            "style": "IPY_MODEL_7c919a8b447d43ea9a4d76fa09e774a9",
            "value": "0.002 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "f0429fea9a07473a931e7605f6308826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33cb4dbda4474a89918493cd802c0741",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d07f9b297744cd7894ee2b06002500a",
            "value": 0.14402724839834563
          }
        },
        "e223bcbb6f5e4c7d95ca5c69512a2913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9cca19f0e242eba0240104347eb433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c919a8b447d43ea9a4d76fa09e774a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33cb4dbda4474a89918493cd802c0741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d07f9b297744cd7894ee2b06002500a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/drug_design_JAK/blob/main/COMPLEX_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RglcWchMSDo",
        "outputId": "1a1874b7-cd4b-4730-b6cf-64fac993344d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# USE GPU, need to connect to gpu\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim --quiet\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html --quiet\n",
        "!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html --quiet\n",
        "!pip install omegaconf --quiet\n",
        "!pip install rdkit --quiet\n",
        "\n",
        "# !pip install rdkit==2022.3.3 --quiet\n",
        "\n",
        "!pip install pubchempy --quiet\n",
        "\n",
        "!pip install torchmetrics --quiet\n",
        "!pip install wandb -qU\n",
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "RQ72ON8jMk5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Restart = True #@param {type:\"boolean\"}\n",
        "import os\n",
        "if Restart:\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "KWTaPQzD2DAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/A_JAK_design"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9P7mM48MdJi",
        "outputId": "58aec03c-924b-4e28-85a4-a9b8ff7700df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/A_JAK_design\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "from help_function.package_version_check import *\n",
        "from help_function.function import *\n",
        "# main()\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7blXUtXgNKX3",
        "outputId": "7496d48f-21ee-4a21-ae07-c421c1281867"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9zwjL2z3j8w",
        "outputId": "22bca60a-e659-44cf-e141-e33872598582"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version:  2.0.1+cu118\n",
            "cuda available:  True\n",
            "rdkit version:  2022.03.3\n",
            "matplotlib version:  3.7.1\n",
            "dgl version:  1.1.1+cu118\n",
            "did not install dgllife, required by MTATFP\n",
            "gensim version:  4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "print(torchmetrics.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-BJz1Dd3htS",
        "outputId": "2cbfe035-e4af-477e-a744-4c2c52f8792a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "71v5YKTV3zLI",
        "outputId": "bc99bc46-dd08-4037-f127-c137fc63c7d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play with wandb"
      ],
      "metadata": {
        "id": "7FI72xa24GJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Launch 5 simulated experiments\n",
        "total_runs = 5\n",
        "for run in range(total_runs):\n",
        "  # 🐝 1️⃣ Start a new run to track this script\n",
        "  wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"basic-intro\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"experiment_{run}\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"learning_rate\": 0.02,\n",
        "      \"architecture\": \"CNN\",\n",
        "      \"dataset\": \"CIFAR-100\",\n",
        "      \"epochs\": 10,\n",
        "      })\n",
        "\n",
        "  # This simple block simulates a training loop logging metrics\n",
        "  epochs = 10\n",
        "  offset = random.random() / 5\n",
        "  for epoch in range(2, epochs):\n",
        "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "\n",
        "      # 🐝 2️⃣ Log metrics from your script to W&B\n",
        "      wandb.log({\"acc\": acc, \"loss\": loss})\n",
        "\n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "kj8KQrdf4Fv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title\n",
        "import wandb\n",
        "import math\n",
        "import random\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def get_dataloader(is_train, batch_size, slice=5):\n",
        "    \"Get a training dataloader\"\n",
        "    full_dataset = torchvision.datasets.MNIST(root=\".\", train=is_train, transform=T.ToTensor(), download=True)\n",
        "    sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))\n",
        "    loader = torch.utils.data.DataLoader(dataset=sub_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True if is_train else False,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "def get_model(dropout):\n",
        "    \"A simple model\"\n",
        "    model = nn.Sequential(nn.Flatten(),\n",
        "                         nn.Linear(28*28, 256),\n",
        "                         nn.BatchNorm1d(256),\n",
        "                         nn.ReLU(),\n",
        "                         nn.Dropout(dropout),\n",
        "                         nn.Linear(256,10)).to(device)\n",
        "    return model\n",
        "\n",
        "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
        "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
        "    model.eval()\n",
        "    val_loss = 0.\n",
        "    with torch.inference_mode():\n",
        "        correct = 0\n",
        "        for i, (images, labels) in enumerate(valid_dl):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass ➡\n",
        "            outputs = model(images)\n",
        "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
        "\n",
        "            # Compute accuracy and accumulate\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Log one batch of images to the dashboard, always same batch_idx.\n",
        "            if i==batch_idx and log_images:\n",
        "                log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
        "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
        "\n",
        "def log_image_table(images, predicted, labels, probs):\n",
        "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
        "    # 🐝 Create a wandb Table to log images, labels and predictions to\n",
        "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\n",
        "    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\n",
        "        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
        "    wandb.log({\"predictions_table\":table}, commit=False)"
      ],
      "metadata": {
        "id": "n4lGrpFi4hZK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch 5 experiments, trying different dropout rates\n",
        "for _ in range(5):\n",
        "    # 🐝 initialise a wandb run\n",
        "    wandb.init(\n",
        "        project=\"pytorch-intro\",\n",
        "        config={\n",
        "            \"epochs\": 10,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 1e-3,\n",
        "            \"dropout\": random.uniform(0.01, 0.80),\n",
        "            })\n",
        "\n",
        "    # Copy your config\n",
        "    config = wandb.config\n",
        "\n",
        "    # Get the data\n",
        "    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
        "    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
        "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
        "\n",
        "    # A simple MLP model\n",
        "    model = get_model(config.dropout)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "   # Training\n",
        "    example_ct = 0\n",
        "    step_ct = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        for step, (images, labels) in enumerate(train_dl):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            train_loss = loss_func(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            example_ct += len(images)\n",
        "            metrics = {\"train/train_loss\": train_loss,\n",
        "                       \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n",
        "                       \"train/example_ct\": example_ct}\n",
        "\n",
        "            if step + 1 < n_steps_per_epoch:\n",
        "                # 🐝 Log train metrics to wandb\n",
        "                wandb.log(metrics)\n",
        "\n",
        "            step_ct += 1\n",
        "\n",
        "        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))\n",
        "\n",
        "        # 🐝 Log train and validation metrics to wandb\n",
        "        val_metrics = {\"val/val_loss\": val_loss,\n",
        "                       \"val/val_accuracy\": accuracy}\n",
        "        wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # If you had a test set, this is how you could log it as a Summary metric\n",
        "    wandb.summary['test_accuracy'] = 0.8\n",
        "\n",
        "    # 🐝 Close your wandb run\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "315fc6b2b6ef49f1b3623954c557c8ee",
            "e7ec6810671d4376bd0506a46480d1c6",
            "ef68a98b12004437802f3e40434a8758",
            "27fda19c7a74412987ed09935fd64f18",
            "ffb608c9818e4294a1ae622e31791239",
            "4f1a88516b464375a5dfe1a36868bb9e",
            "370488654a9e496c8ec06121e9e54aa6",
            "d0f3d7252b2147a284b7698b3d7c166c",
            "0ec40334785149f6950b9137a24c2121",
            "0c39e04ca4174939b9119f992452103f",
            "8748a12eb42446c4a2c4186c51aa9d22",
            "007f7db9c4ec469182d950815d0c00d8",
            "ba901ff31ae8464c93f13a04ca8f4d23",
            "4610d571da374ae396eacacecc783733",
            "1d06ebf8bdc3422b89d52fffd1527218",
            "5f005e24b67f4d8f945d1675c622901c"
          ]
        },
        "id": "-zKtgJh-4x76",
        "outputId": "392b09e4-4c1f-423f-f81f-3fc242226dd5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/A_JAK_design/wandb/run-20230726_202803-204os8vt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lanlang/pytorch-intro/runs/204os8vt' target=\"_blank\">mild-planet-1</a></strong> to <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lanlang/pytorch-intro/runs/204os8vt' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/204os8vt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 199435456.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 26605687.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 62257653.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3634209.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.278, Valid Loss: 0.303901, Accuracy: 0.91\n",
            "Train Loss: 0.300, Valid Loss: 0.241614, Accuracy: 0.93\n",
            "Train Loss: 0.121, Valid Loss: 0.212065, Accuracy: 0.93\n",
            "Train Loss: 0.188, Valid Loss: 0.202937, Accuracy: 0.94\n",
            "Train Loss: 0.175, Valid Loss: 0.192227, Accuracy: 0.94\n",
            "Train Loss: 0.144, Valid Loss: 0.180179, Accuracy: 0.94\n",
            "Train Loss: 0.202, Valid Loss: 0.180104, Accuracy: 0.95\n",
            "Train Loss: 0.134, Valid Loss: 0.166578, Accuracy: 0.95\n",
            "Train Loss: 0.079, Valid Loss: 0.166934, Accuracy: 0.95\n",
            "Train Loss: 0.082, Valid Loss: 0.166034, Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▅▄▄▃▂▃▂▂▃▂▁▂▂▁▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▂</td></tr><tr><td>val/val_accuracy</td><td>▁▄▅▅▆▆▇▇█▇</td></tr><tr><td>val/val_loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>120000</td></tr><tr><td>train/train_loss</td><td>0.08244</td></tr><tr><td>val/val_accuracy</td><td>0.9475</td></tr><tr><td>val/val_loss</td><td>0.16603</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">mild-planet-1</strong> at: <a href='https://wandb.ai/lanlang/pytorch-intro/runs/204os8vt' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/204os8vt</a><br/>Synced 5 W&B file(s), 1 media file(s), 257 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230726_202803-204os8vt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668816816665336, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "315fc6b2b6ef49f1b3623954c557c8ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/A_JAK_design/wandb/run-20230726_202911-jup96lc9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lanlang/pytorch-intro/runs/jup96lc9' target=\"_blank\">vague-donkey-2</a></strong> to <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lanlang/pytorch-intro/runs/jup96lc9' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/jup96lc9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.283, Valid Loss: 0.287879, Accuracy: 0.92\n",
            "Train Loss: 0.155, Valid Loss: 0.231149, Accuracy: 0.93\n",
            "Train Loss: 0.153, Valid Loss: 0.192998, Accuracy: 0.94\n",
            "Train Loss: 0.130, Valid Loss: 0.185462, Accuracy: 0.94\n",
            "Train Loss: 0.154, Valid Loss: 0.172239, Accuracy: 0.95\n",
            "Train Loss: 0.125, Valid Loss: 0.160574, Accuracy: 0.95\n",
            "Train Loss: 0.046, Valid Loss: 0.156353, Accuracy: 0.95\n",
            "Train Loss: 0.047, Valid Loss: 0.152121, Accuracy: 0.95\n",
            "Train Loss: 0.059, Valid Loss: 0.151564, Accuracy: 0.96\n",
            "Train Loss: 0.027, Valid Loss: 0.153792, Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_accuracy</td><td>▁▃▅▅▆▇▇███</td></tr><tr><td>val/val_loss</td><td>█▅▃▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>120000</td></tr><tr><td>train/train_loss</td><td>0.02704</td></tr><tr><td>val/val_accuracy</td><td>0.954</td></tr><tr><td>val/val_loss</td><td>0.15379</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vague-donkey-2</strong> at: <a href='https://wandb.ai/lanlang/pytorch-intro/runs/jup96lc9' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/jup96lc9</a><br/>Synced 5 W&B file(s), 1 media file(s), 257 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230726_202911-jup96lc9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671118833335184, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ec40334785149f6950b9137a24c2121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/A_JAK_design/wandb/run-20230726_203009-0u05ji90</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lanlang/pytorch-intro/runs/0u05ji90' target=\"_blank\">robust-sponge-3</a></strong> to <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lanlang/pytorch-intro/runs/0u05ji90' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/0u05ji90</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.239, Valid Loss: 0.292383, Accuracy: 0.91\n",
            "Train Loss: 0.249, Valid Loss: 0.238141, Accuracy: 0.93\n",
            "Train Loss: 0.302, Valid Loss: 0.201999, Accuracy: 0.94\n",
            "Train Loss: 0.275, Valid Loss: 0.179764, Accuracy: 0.94\n",
            "Train Loss: 0.118, Valid Loss: 0.175648, Accuracy: 0.94\n",
            "Train Loss: 0.073, Valid Loss: 0.170724, Accuracy: 0.95\n",
            "Train Loss: 0.114, Valid Loss: 0.170584, Accuracy: 0.94\n",
            "Train Loss: 0.101, Valid Loss: 0.161808, Accuracy: 0.94\n",
            "Train Loss: 0.089, Valid Loss: 0.157399, Accuracy: 0.95\n",
            "Train Loss: 0.077, Valid Loss: 0.149108, Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▆▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▂▁</td></tr><tr><td>val/val_accuracy</td><td>▁▄▅▆▇█▇▇▇█</td></tr><tr><td>val/val_loss</td><td>█▅▄▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>120000</td></tr><tr><td>train/train_loss</td><td>0.07687</td></tr><tr><td>val/val_accuracy</td><td>0.95</td></tr><tr><td>val/val_loss</td><td>0.14911</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-sponge-3</strong> at: <a href='https://wandb.ai/lanlang/pytorch-intro/runs/0u05ji90' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/0u05ji90</a><br/>Synced 5 W&B file(s), 1 media file(s), 257 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230726_203009-0u05ji90/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/A_JAK_design/wandb/run-20230726_203104-5ai1j89k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lanlang/pytorch-intro/runs/5ai1j89k' target=\"_blank\">comfy-planet-4</a></strong> to <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lanlang/pytorch-intro/runs/5ai1j89k' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/5ai1j89k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.275, Valid Loss: 0.275531, Accuracy: 0.92\n",
            "Train Loss: 0.206, Valid Loss: 0.228441, Accuracy: 0.93\n",
            "Train Loss: 0.245, Valid Loss: 0.195843, Accuracy: 0.94\n",
            "Train Loss: 0.172, Valid Loss: 0.170608, Accuracy: 0.94\n",
            "Train Loss: 0.076, Valid Loss: 0.175937, Accuracy: 0.94\n",
            "Train Loss: 0.110, Valid Loss: 0.161026, Accuracy: 0.94\n",
            "Train Loss: 0.076, Valid Loss: 0.157138, Accuracy: 0.95\n",
            "Train Loss: 0.016, Valid Loss: 0.153006, Accuracy: 0.95\n",
            "Train Loss: 0.076, Valid Loss: 0.148194, Accuracy: 0.95\n",
            "Train Loss: 0.035, Valid Loss: 0.145624, Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▅▃▄▃▄▂▃▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_accuracy</td><td>▁▃▅▅▆▆▇▇▇█</td></tr><tr><td>val/val_loss</td><td>█▅▄▂▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>120000</td></tr><tr><td>train/train_loss</td><td>0.03546</td></tr><tr><td>val/val_accuracy</td><td>0.9545</td></tr><tr><td>val/val_loss</td><td>0.14562</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comfy-planet-4</strong> at: <a href='https://wandb.ai/lanlang/pytorch-intro/runs/5ai1j89k' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/5ai1j89k</a><br/>Synced 5 W&B file(s), 1 media file(s), 257 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230726_203104-5ai1j89k/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/A_JAK_design/wandb/run-20230726_203156-1tlcgqj2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lanlang/pytorch-intro/runs/1tlcgqj2' target=\"_blank\">dark-sun-5</a></strong> to <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lanlang/pytorch-intro' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lanlang/pytorch-intro/runs/1tlcgqj2' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/1tlcgqj2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.374, Valid Loss: 0.312494, Accuracy: 0.91\n",
            "Train Loss: 0.295, Valid Loss: 0.251874, Accuracy: 0.93\n",
            "Train Loss: 0.288, Valid Loss: 0.224165, Accuracy: 0.93\n",
            "Train Loss: 0.346, Valid Loss: 0.212923, Accuracy: 0.94\n",
            "Train Loss: 0.153, Valid Loss: 0.189687, Accuracy: 0.94\n",
            "Train Loss: 0.100, Valid Loss: 0.185704, Accuracy: 0.94\n",
            "Train Loss: 0.163, Valid Loss: 0.185833, Accuracy: 0.94\n",
            "Train Loss: 0.186, Valid Loss: 0.175903, Accuracy: 0.94\n",
            "Train Loss: 0.227, Valid Loss: 0.168774, Accuracy: 0.94\n",
            "Train Loss: 0.159, Valid Loss: 0.163359, Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▂▁▁▁</td></tr><tr><td>val/val_accuracy</td><td>▁▅▅▆▇▇▇███</td></tr><tr><td>val/val_loss</td><td>█▅▄▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>120000</td></tr><tr><td>train/train_loss</td><td>0.15936</td></tr><tr><td>val/val_accuracy</td><td>0.9465</td></tr><tr><td>val/val_loss</td><td>0.16336</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dark-sun-5</strong> at: <a href='https://wandb.ai/lanlang/pytorch-intro/runs/1tlcgqj2' target=\"_blank\">https://wandb.ai/lanlang/pytorch-intro/runs/1tlcgqj2</a><br/>Synced 5 W&B file(s), 1 media file(s), 257 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230726_203156-1tlcgqj2/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a wandb run\n",
        "wandb.init(project=\"pytorch-intro\")\n",
        "\n",
        "# Simulating a model training loop\n",
        "acc_threshold = 0.3\n",
        "for training_step in range(1000):\n",
        "\n",
        "    # Generate a random number for accuracy\n",
        "    accuracy = round(random.random() + random.random(), 3)\n",
        "    print(f'Accuracy is: {accuracy}, {acc_threshold}')\n",
        "\n",
        "    # 🐝 Log accuracy to wandb\n",
        "    wandb.log({\"Accuracy\": accuracy})\n",
        "\n",
        "    # 🔔 If the accuracy is below the threshold, fire a W&B Alert and stop the run\n",
        "    if accuracy <= acc_threshold:\n",
        "        # 🐝 Send the wandb Alert\n",
        "        wandb.alert(\n",
        "            title='Low Accuracy',\n",
        "            text=f'Accuracy {accuracy} at step {training_step} is below the acceptable theshold, {acc_threshold}',\n",
        "        )\n",
        "        print('Alert triggered')\n",
        "        break\n",
        "\n",
        "# Mark the run as finished (useful in Jupyter notebooks)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "iuf34tHj5xCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9q8g9gy8qg6",
        "outputId": "3841ecd8-37fd-4af0-c279-2f49b852e9a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/722.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/722.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m573.4/722.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://tdcommons.ai/start/\n",
        "!pip install PyTDC --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZKfT7wl9Iyq",
        "outputId": "cec564ac-c744-4b48-f130-172a477e8678"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for PyTDC (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing as T\n",
        "\n",
        "import logging as lg\n",
        "import multiprocessing as mp\n",
        "import sys\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "from tqdm import tqdm\n",
        "\n",
        "logLevels = {0: lg.ERROR, 1: lg.WARNING, 2: lg.INFO, 3: lg.DEBUG}\n",
        "LOGGER_NAME = \"DTI\"\n",
        "\n",
        "\n",
        "def get_logger(logger_name: str = None) -> lg.Logger:\n",
        "    if logger_name is None:\n",
        "        logger_name = LOGGER_NAME\n",
        "    return lg.getLogger(logger_name)\n",
        "\n",
        "\n",
        "logg = get_logger()\n",
        "\n",
        "\n",
        "def config_logger(\n",
        "    file: T.Union[Path, None],\n",
        "    fmt: str,\n",
        "    level: bool = 2,\n",
        "    use_stdout: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create and configure the logger\n",
        "\n",
        "    :param file: Can be a Path or None -- if a Path, log messages will be written to the file at Path\n",
        "    :type file: T.Union[Path, None]\n",
        "    :param fmt: Formatting string for the log messages\n",
        "    :type fmt: str\n",
        "    :param level: Level of verbosity\n",
        "    :type level: int\n",
        "    :param use_stdout: Whether to also log messages to stdout\n",
        "    :type use_stdout: bool\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    module_logger = lg.getLogger(LOGGER_NAME)\n",
        "    module_logger.setLevel(logLevels[level])\n",
        "    formatter = lg.Formatter(fmt)\n",
        "\n",
        "    if file is not None:\n",
        "        fh = lg.FileHandler(file)\n",
        "        fh.setFormatter(formatter)\n",
        "        module_logger.addHandler(fh)\n",
        "\n",
        "    if use_stdout:\n",
        "        sh = lg.StreamHandler(sys.stdout)\n",
        "        sh.setFormatter(formatter)\n",
        "        module_logger.addHandler(sh)\n",
        "\n",
        "    lg.propagate = False\n",
        "\n",
        "    return module_logger\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def canonicalize(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        return Chem.MolToSmiles(mol, isomericSmiles=True)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def smiles2morgan(s, radius=2, nBits=2048):\n",
        "    \"\"\"\n",
        "    Convert smiles into Morgan Fingerprint.\n",
        "    :param smile: SMILES string\n",
        "    :type smile: str\n",
        "    :return: Morgan fingerprint\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    try:\n",
        "        s = canonicalize(s)\n",
        "        mol = Chem.MolFromSmiles(s)\n",
        "        features_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
        "        features = np.zeros((1,))\n",
        "        DataStructs.ConvertToNumpyArray(features_vec, features)\n",
        "    except Exception as e:\n",
        "        logg.error(e)\n",
        "        logg.error(\n",
        "            f\"Failed to convert SMILES to Morgan Fingerprint: {s} convert to all 0 features\"\n",
        "        )\n",
        "        features = np.zeros((nBits,))\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_config(experiment_id, mol_feat, prot_feat):\n",
        "    data_cfg = {\n",
        "        \"batch_size\": 32,\n",
        "        \"num_workers\": 0,\n",
        "        \"precompute\": True,\n",
        "        \"mol_feat\": mol_feat,\n",
        "        \"prot_feat\": prot_feat,\n",
        "    }\n",
        "    model_cfg = {\n",
        "        \"latent_size\": 1024,\n",
        "    }\n",
        "    training_cfg = {\n",
        "        \"n_epochs\": 50,\n",
        "        \"every_n_val\": 1,\n",
        "    }\n",
        "    cfg = {\n",
        "        \"data\": data_cfg,\n",
        "        \"model\": model_cfg,\n",
        "        \"training\": training_cfg,\n",
        "        \"experiment_id\": experiment_id,\n",
        "    }\n",
        "\n",
        "    return OmegaConf.structured(cfg)\n",
        "\n",
        "\n",
        "def _hdf5_load_partial_func(k, file_path):\n",
        "    \"\"\"\n",
        "    Helper function for load_hdf5_parallel\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(file_path, \"r\") as fi:\n",
        "        emb = torch.from_numpy(fi[k][:])\n",
        "    return emb\n",
        "\n",
        "\n",
        "def load_hdf5_parallel(file_path, keys, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Load keys from hdf5 file into memory\n",
        "    :param file_path: Path to hdf5 file\n",
        "    :type file_path: str\n",
        "    :param keys: List of keys to get\n",
        "    :type keys: list[str]\n",
        "    :return: Dictionary with keys and records in memory\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
        "\n",
        "    if (n_jobs == -1) or (n_jobs > mp.cpu_count()):\n",
        "        n_jobs = mp.cpu_count()\n",
        "\n",
        "    with mp.Pool(processes=n_jobs) as pool:\n",
        "        all_embs = list(\n",
        "            tqdm(\n",
        "                pool.imap(partial(_hdf5_load_partial_func, file_path=file_path), keys),\n",
        "                total=len(keys),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    embeddings = {k: v for k, v in zip(keys, all_embs)}\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "class Cosine(nn.Module):\n",
        "    def forward(self, x1, x2):\n",
        "        return nn.CosineSimilarity()(x1, x2)\n",
        "\n",
        "class SquaredCosine(nn.Module):\n",
        "    def forward(self, x1, x2):\n",
        "        return nn.CosineSimilarity()(x1, x2) ** 2\n",
        "\n",
        "class Euclidean(nn.Module):\n",
        "    def forward(self, x1, x2):\n",
        "        return torch.cdist(x1, x2, p=2.0)\n",
        "\n",
        "\n",
        "class SquaredEuclidean(nn.Module):\n",
        "    def forward(self, x1, x2):\n",
        "        return torch.cdist(x1, x2, p=2.0) ** 2\n",
        "\n",
        "\n",
        "#######################\n",
        "# Model Architectures #\n",
        "#######################\n",
        "\n",
        "\n",
        "DISTANCE_METRICS = {\n",
        "    \"Cosine\": Cosine,\n",
        "    \"SquaredCosine\": SquaredCosine,\n",
        "    \"Euclidean\": Euclidean,\n",
        "    \"SquaredEuclidean\": SquaredEuclidean,\n",
        "}\n",
        "\n",
        "ACTIVATIONS = {\"ReLU\": nn.ReLU, \"GELU\": nn.GELU, \"ELU\": nn.ELU, \"Sigmoid\": nn.Sigmoid}\n"
      ],
      "metadata": {
        "id": "4LZ7snEf9ayZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import annotations\n",
        "\n",
        "import typing as T\n",
        "\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "# from src.featurizers.molecule import MorganFeaturizer\n",
        "\n",
        "# import MorganFeaturizer, ProtBertFeaturizer\n",
        "# from model.architectures import SimpleCoembeddingNoSigmoid\n",
        "\n",
        "class Featurizer:\n",
        "    def __init__(\n",
        "        self, name: str, shape: int, save_dir: Path = Path().absolute()\n",
        "    ):\n",
        "        self._name = name\n",
        "        self._shape = shape\n",
        "        self._save_path = save_dir / Path(f\"{self._name}_features.h5\")\n",
        "\n",
        "        self._preloaded = False\n",
        "        self._device = torch.device(\"cpu\")\n",
        "        self._cuda_registry = {}\n",
        "        self._on_cuda = False\n",
        "        self._features = {}\n",
        "\n",
        "    def __call__(self, seq: str) -> torch.Tensor:\n",
        "        if seq not in self.features:\n",
        "            self._features[seq] = self.transform(seq)\n",
        "\n",
        "        return self._features[seq]\n",
        "\n",
        "    def _register_cuda(self, k: str, v, f=None):\n",
        "        \"\"\"\n",
        "        Register an object as capable of being moved to a CUDA device\n",
        "        \"\"\"\n",
        "        self._cuda_registry[k] = (v, f)\n",
        "\n",
        "    def _transform(self, seq: str) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _update_device(self, device: torch.device):\n",
        "        self._device = device\n",
        "        for k, (v, f) in self._cuda_registry.items():\n",
        "            if f is None:\n",
        "                try:\n",
        "                    self._cuda_registry[k] = (v.to(self._device), None)\n",
        "                except RuntimeError as e:\n",
        "                    logg.error(e)\n",
        "                    logg.debug(device)\n",
        "                    logg.debug(type(self._device))\n",
        "                    logg.debug(self._device)\n",
        "            else:\n",
        "                self._cuda_registry[k] = (f(v, self._device), f)\n",
        "        for k, v in self._features.items():\n",
        "            self._features[k] = v.to(device)\n",
        "\n",
        "    @lru_cache(maxsize=5000)\n",
        "    def transform(self, seq: str) -> torch.Tensor:\n",
        "        with torch.set_grad_enabled(False):\n",
        "            feats = self._transform(seq)\n",
        "            if self._on_cuda:\n",
        "                feats = feats.to(self.device)\n",
        "            return feats\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return self._name\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> int:\n",
        "        return self._shape\n",
        "\n",
        "    @property\n",
        "    def path(self) -> Path:\n",
        "        return self._save_path\n",
        "\n",
        "    @property\n",
        "    def features(self) -> dict:\n",
        "        return self._features\n",
        "\n",
        "    @property\n",
        "    def on_cuda(self) -> bool:\n",
        "        return self._on_cuda\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return self._device\n",
        "\n",
        "    def to(self, device: torch.device) -> Featurizer:\n",
        "        self._update_device(device)\n",
        "        self._on_cuda = device.type == \"cuda\"\n",
        "        return self\n",
        "\n",
        "    def cuda(self, device: torch.device) -> Featurizer:\n",
        "        \"\"\"\n",
        "        Perform model computations on CUDA, move saved embeddings to CUDA device\n",
        "        \"\"\"\n",
        "        self._update_device(device)\n",
        "        self._on_cuda = True\n",
        "        return self\n",
        "\n",
        "    def cpu(self) -> Featurizer:\n",
        "        \"\"\"\n",
        "        Perform model computations on CPU, move saved embeddings to CPU\n",
        "        \"\"\"\n",
        "        self._update_device(torch.device(\"cpu\"))\n",
        "        self._on_cuda = False\n",
        "        return self\n",
        "\n",
        "    def write_to_disk(\n",
        "        self, seq_list: T.List[str], verbose: bool = True\n",
        "    ) -> None:\n",
        "        logg.info(f\"Writing {self.name} features to {self.path}\")\n",
        "        with h5py.File(self._save_path, \"a\") as h5fi:\n",
        "            for seq in tqdm(seq_list, disable=not verbose, desc=self.name):\n",
        "                seq_h5 = sanitize_string(seq)\n",
        "                if seq_h5 in h5fi:\n",
        "                    logg.warning(f\"{seq} already in h5file\")\n",
        "                feats = self.transform(seq)\n",
        "                dset = h5fi.require_dataset(seq_h5, feats.shape, np.float32)\n",
        "                dset[:] = feats.cpu().numpy()\n",
        "\n",
        "    def preload(\n",
        "        self,\n",
        "        seq_list: T.List[str],\n",
        "        verbose: bool = True,\n",
        "        write_first: bool = True,\n",
        "    ) -> None:\n",
        "        logg.info(f\"Preloading {self.name} features from {self.path}\")\n",
        "\n",
        "        if write_first and not self._save_path.exists():\n",
        "            self.write_to_disk(seq_list, verbose=verbose)\n",
        "\n",
        "        if self._save_path.exists():\n",
        "            with h5py.File(self._save_path, \"r\") as h5fi:\n",
        "                for seq in tqdm(seq_list, disable=not verbose, desc=self.name):\n",
        "                    if seq in h5fi:\n",
        "                        seq_h5 = sanitize_string(seq)\n",
        "                        feats = torch.from_numpy(h5fi[seq_h5][:])\n",
        "                    else:\n",
        "                        feats = self.transform(seq)\n",
        "\n",
        "                    if self._on_cuda:\n",
        "                        feats = feats.to(self.device)\n",
        "\n",
        "                    self._features[seq] = feats\n",
        "\n",
        "        else:\n",
        "            for seq in tqdm(seq_list, disable=not verbose, desc=self.name):\n",
        "                feats = self.transform(seq)\n",
        "\n",
        "                if self._on_cuda:\n",
        "                    feats = feats.to(self.device)\n",
        "\n",
        "                self._features[seq] = feats\n",
        "\n",
        "        # seqs_sanitized = [sanitize_string(s) for s in seq_list]\n",
        "        # feat_dict = load_hdf5_parallel(self._save_path, seqs_sanitized,n_jobs=32)\n",
        "        # self._features.update(feat_dict)\n",
        "\n",
        "        self._update_device(self.device)\n",
        "        self._preloaded = True\n",
        "\n",
        "class MorganFeaturizer(Featurizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        shape: int = 2048,\n",
        "        radius: int = 2,\n",
        "        save_dir: Path = Path().absolute(),\n",
        "    ):\n",
        "        super().__init__(\"Morgan\", shape, save_dir)\n",
        "\n",
        "        self._radius = radius\n",
        "\n",
        "    def smiles_to_morgan(self, smile: str):\n",
        "        \"\"\"\n",
        "        Convert smiles into Morgan Fingerprint.\n",
        "        :param smile: SMILES string\n",
        "        :type smile: str\n",
        "        :return: Morgan fingerprint\n",
        "        :rtype: np.ndarray\n",
        "        \"\"\"\n",
        "        try:\n",
        "            smile = canonicalize(smile)\n",
        "            mol = Chem.MolFromSmiles(smile)\n",
        "            features_vec = AllChem.GetMorganFingerprintAsBitVect(\n",
        "                mol, self._radius, nBits=self.shape\n",
        "            )\n",
        "            features = np.zeros((1,))\n",
        "            DataStructs.ConvertToNumpyArray(features_vec, features)\n",
        "        except Exception as e:\n",
        "            logg.error(\n",
        "                f\"rdkit not found this smiles for morgan: {smile} convert to all 0 features\"\n",
        "            )\n",
        "            logg.error(e)\n",
        "            features = np.zeros((self.shape,))\n",
        "        return features\n",
        "\n",
        "    def _transform(self, smile: str) -> torch.Tensor:\n",
        "        # feats = torch.from_numpy(self._featurizer(smile)).squeeze().float()\n",
        "        feats = (\n",
        "            torch.from_numpy(self.smiles_to_morgan(smile)).squeeze().float()\n",
        "        )\n",
        "        if feats.shape[0] != self.shape:\n",
        "            logg.warning(\"Failed to featurize: appending zero vector\")\n",
        "            feats = torch.zeros(self.shape)\n",
        "        return feats\n",
        "\n",
        "\n",
        "class SimpleCoembedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "class SimpleCoembeddingSigmoid(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        sigmoid_f = torch.nn.Sigmoid()\n",
        "        return sigmoid_f(distance).squeeze()\n"
      ],
      "metadata": {
        "id": "vdRsqX6J93pm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import typing as T\n",
        "# from types import SimpleNamespace\n",
        "\n",
        "# import os\n",
        "# import pickle as pk\n",
        "# import sys\n",
        "# from functools import lru_cache\n",
        "# from pathlib import Path\n",
        "\n",
        "# import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from numpy.random import choice\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tdc.benchmark_group import dti_dg_group\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# from ..featurizer import Featurizer\n",
        "# from ..featurizer.protein import FOLDSEEK_MISSING_IDX\n",
        "# from ..utils import get_logger\n",
        "\n",
        "logg = get_logger()\n",
        "\n",
        "\n",
        "def get_task_dir(task_name: str, database_root: Path):\n",
        "    \"\"\"\n",
        "    Get the path to data for each benchmark data set\n",
        "\n",
        "    :param task_name: Name of benchmark\n",
        "    :type task_name: str\n",
        "    \"\"\"\n",
        "\n",
        "    database_root = Path(database_root).resolve()\n",
        "\n",
        "    task_paths = {\n",
        "        \"biosnap\": database_root / \"BIOSNAP/full_data\",\n",
        "        \"biosnap_prot\": database_root / \"BIOSNAP/unseen_protein\",\n",
        "        \"biosnap_mol\": database_root / \"BIOSNAP/unseen_drug\",\n",
        "        \"bindingdb\": database_root / \"BindingDB\",\n",
        "        \"davis\": database_root / \"DAVIS\",\n",
        "        \"dti_dg\": database_root / \"TDC\",\n",
        "        \"dude\": database_root / \"DUDe\",\n",
        "        \"halogenase\": database_root / \"EnzPred/halogenase_NaCl_binary\",\n",
        "        \"bkace\": database_root / \"EnzPred/duf_binary\",\n",
        "        \"gt\": database_root / \"EnzPred/gt_acceptors_achiral_binary\",\n",
        "        \"esterase\": database_root / \"EnzPred/esterase_binary\",\n",
        "        \"kinase\": database_root / \"EnzPred/davis_filtered\",\n",
        "        \"phosphatase\": database_root / \"EnzPred/phosphatase_chiral_binary\",\n",
        "    }\n",
        "\n",
        "    return Path(task_paths[task_name.lower()]).resolve()\n",
        "\n",
        "\n",
        "def drug_target_collate_fn(args: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
        "    \"\"\"\n",
        "    Collate function for PyTorch data loader -- turn a batch of triplets into a triplet of batches\n",
        "\n",
        "    If target embeddings are not all the same length, it will zero pad them\n",
        "    This is to account for differences in length from FoldSeek embeddings\n",
        "\n",
        "    :param args: Batch of training samples with molecule, protein, and affinity\n",
        "    :type args: Iterable[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]\n",
        "    :return: Create a batch of examples\n",
        "    :rtype: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "    \"\"\"\n",
        "    d_emb = [a[0] for a in args]\n",
        "    t_emb = [a[1] for a in args]\n",
        "    labs = [a[2] for a in args]\n",
        "\n",
        "    drugs = torch.stack(d_emb, 0)\n",
        "    targets = pad_sequence(t_emb, batch_first=True, padding_value=FOLDSEEK_MISSING_IDX)\n",
        "    labels = torch.stack(labs, 0)\n",
        "\n",
        "    return drugs, targets, labels\n",
        "\n",
        "\n",
        "def contrastive_collate_fn(args: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
        "    \"\"\"\n",
        "    Collate function for PyTorch data loader -- turn a batch of triplets into a triplet of batches\n",
        "\n",
        "    Specific collate function for contrastive dataloader\n",
        "\n",
        "    :param args: Batch of training samples with anchor, positive, negative\n",
        "    :type args: Iterable[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]\n",
        "    :return: Create a batch of examples\n",
        "    :rtype: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "    \"\"\"\n",
        "    anchor_emb = [a[0] for a in args]\n",
        "    pos_emb = [a[1] for a in args]\n",
        "    neg_emb = [a[2] for a in args]\n",
        "\n",
        "    anchors = pad_sequence(\n",
        "        anchor_emb, batch_first=True, padding_value=FOLDSEEK_MISSING_IDX\n",
        "    )\n",
        "    positives = torch.stack(pos_emb, 0)\n",
        "    negatives = torch.stack(neg_emb, 0)\n",
        "\n",
        "    return anchors, positives, negatives\n",
        "\n",
        "\n",
        "def make_contrastive(\n",
        "    df: pd.DataFrame,\n",
        "    posneg_column: str,\n",
        "    anchor_column: str,\n",
        "    label_column: str,\n",
        "    n_neg_per: int = 50,):\n",
        "    pos_df = df[df[label_column] == 1]\n",
        "    neg_df = df[df[label_column] == 0]\n",
        "\n",
        "    contrastive = []\n",
        "\n",
        "    for _, r in pos_df.iterrows():\n",
        "        for _ in range(n_neg_per):\n",
        "            contrastive.append(\n",
        "                (\n",
        "                    r[anchor_column],\n",
        "                    r[posneg_column],\n",
        "                    choice(neg_df[posneg_column]),\n",
        "                )\n",
        "            )\n",
        "\n",
        "    contrastive = pd.DataFrame(contrastive, columns=[\"Anchor\", \"Positive\", \"Negative\"])\n",
        "    return contrastive\n",
        "\n",
        "\n",
        "class BinaryDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drugs,\n",
        "        targets,\n",
        "        labels,\n",
        "        drug_featurizer: Featurizer,\n",
        "        target_featurizer: Featurizer,\n",
        "    ):\n",
        "        self.drugs = drugs\n",
        "        self.targets = targets\n",
        "        self.labels = labels\n",
        "\n",
        "        self.drug_featurizer = drug_featurizer\n",
        "        self.target_featurizer = target_featurizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.drugs)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        drug = self.drug_featurizer(self.drugs.iloc[i])\n",
        "        target = self.target_featurizer(self.targets.iloc[i])\n",
        "        label = torch.tensor(self.labels.iloc[i])\n",
        "\n",
        "        return drug, target, label\n",
        "\n",
        "\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        anchors,\n",
        "        positives,\n",
        "        negatives,\n",
        "        posneg_featurizer: Featurizer,\n",
        "        anchor_featurizer: Featurizer,\n",
        "    ):\n",
        "        self.anchors = anchors\n",
        "        self.positives = positives\n",
        "        self.negatives = negatives\n",
        "\n",
        "        self.posneg_featurizer = posneg_featurizer\n",
        "        self.anchor_featurizer = anchor_featurizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchors)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        anchorEmb = self.anchor_featurizer(self.anchors[i])\n",
        "        positiveEmb = self.posneg_featurizer(self.positives[i])\n",
        "        negativeEmb = self.posneg_featurizer(self.negatives[i])\n",
        "\n",
        "        return anchorEmb, positiveEmb, negativeEmb\n",
        "\n",
        "\n",
        "class DTIDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        drug_featurizer: Featurizer,\n",
        "        target_featurizer: Featurizer,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "        batch_size: int = 32,\n",
        "        shuffle: bool = True,\n",
        "        num_workers: int = 0,\n",
        "        header=0,\n",
        "        index_col=0,\n",
        "        sep=\",\",\n",
        "    ):\n",
        "        self._loader_kwargs = {\n",
        "            \"batch_size\": batch_size,\n",
        "            \"shuffle\": shuffle,\n",
        "            \"num_workers\": num_workers,\n",
        "            \"collate_fn\": drug_target_collate_fn,\n",
        "        }\n",
        "\n",
        "        self._csv_kwargs = {\n",
        "            \"header\": header,\n",
        "            \"index_col\": index_col,\n",
        "            \"sep\": sep,\n",
        "        }\n",
        "\n",
        "        self._device = device\n",
        "\n",
        "        self._data_dir = Path(data_dir)\n",
        "        self._train_path = Path(\"train.csv\")\n",
        "        self._val_path = Path(\"val.csv\")\n",
        "        self._test_path = Path(\"test.csv\")\n",
        "\n",
        "        self._drug_column = \"SMILES\"\n",
        "        self._target_column = \"Target Sequence\"\n",
        "        self._label_column = \"Label\"\n",
        "\n",
        "        self.drug_featurizer = drug_featurizer\n",
        "        self.target_featurizer = target_featurizer\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if self.drug_featurizer.path.exists() and self.target_featurizer.path.exists():\n",
        "            logg.warning(\"Drug and target featurizers already exist\")\n",
        "            return\n",
        "\n",
        "        df_train = pd.read_csv(self._data_dir / self._train_path, **self._csv_kwargs)\n",
        "\n",
        "        df_val = pd.read_csv(self._data_dir / self._val_path, **self._csv_kwargs)\n",
        "\n",
        "        df_test = pd.read_csv(self._data_dir / self._test_path, **self._csv_kwargs)\n",
        "\n",
        "        dataframes = [df_train, df_val, df_test]\n",
        "        all_drugs = pd.concat([i[self._drug_column] for i in dataframes]).unique()\n",
        "        all_targets = pd.concat([i[self._target_column] for i in dataframes]).unique()\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        if not self.drug_featurizer.path.exists():\n",
        "            self.drug_featurizer.write_to_disk(all_drugs)\n",
        "\n",
        "        if not self.target_featurizer.path.exists():\n",
        "            self.target_featurizer.write_to_disk(all_targets)\n",
        "\n",
        "        self.drug_featurizer.cpu()\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "    def setup(self, stage: T.Optional[str] = None):\n",
        "        self.df_train = pd.read_csv(\n",
        "            self._data_dir / self._train_path, **self._csv_kwargs\n",
        "        )\n",
        "\n",
        "        self.df_val = pd.read_csv(self._data_dir / self._val_path, **self._csv_kwargs)\n",
        "\n",
        "        self.df_test = pd.read_csv(self._data_dir / self._test_path, **self._csv_kwargs)\n",
        "\n",
        "        self._dataframes = [self.df_train, self.df_val, self.df_test]\n",
        "\n",
        "        all_drugs = pd.concat([i[self._drug_column] for i in self._dataframes]).unique()\n",
        "        all_targets = pd.concat(\n",
        "            [i[self._target_column] for i in self._dataframes]\n",
        "        ).unique()\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        self.drug_featurizer.preload(all_drugs)\n",
        "        self.drug_featurizer.cpu()\n",
        "\n",
        "        self.target_featurizer.preload(all_targets)\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.data_train = BinaryDataset(\n",
        "                self.df_train[self._drug_column],\n",
        "                self.df_train[self._target_column],\n",
        "                self.df_train[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "            self.data_val = BinaryDataset(\n",
        "                self.df_val[self._drug_column],\n",
        "                self.df_val[self._target_column],\n",
        "                self.df_val[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.data_test = BinaryDataset(\n",
        "                self.df_test[self._drug_column],\n",
        "                self.df_test[self._target_column],\n",
        "                self.df_test[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.data_train, **self._loader_kwargs)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.data_val, **self._loader_kwargs)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.data_test, **self._loader_kwargs)\n",
        "\n",
        "\n",
        "class TDCDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        drug_featurizer: Featurizer,\n",
        "        target_featurizer: Featurizer,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "        seed: int = 0,\n",
        "        batch_size: int = 32,\n",
        "        shuffle: bool = True,\n",
        "        num_workers: int = 0,\n",
        "        header=0,\n",
        "        index_col=0,\n",
        "        sep=\",\",\n",
        "    ):\n",
        "        self._loader_kwargs = {\n",
        "            \"batch_size\": batch_size,\n",
        "            \"shuffle\": shuffle,\n",
        "            \"num_workers\": num_workers,\n",
        "            \"collate_fn\": drug_target_collate_fn,\n",
        "        }\n",
        "\n",
        "        self._csv_kwargs = {\n",
        "            \"header\": header,\n",
        "            \"index_col\": index_col,\n",
        "            \"sep\": sep,\n",
        "        }\n",
        "\n",
        "        self._device = device\n",
        "\n",
        "        self._data_dir = Path(data_dir)\n",
        "        self._seed = seed\n",
        "\n",
        "        self._drug_column = \"Drug\"\n",
        "        self._target_column = \"Target\"\n",
        "        self._label_column = \"Y\"\n",
        "\n",
        "        self.drug_featurizer = drug_featurizer\n",
        "        self.target_featurizer = target_featurizer\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dg_group = dti_dg_group(path=self._data_dir)\n",
        "        dg_benchmark = dg_group.get(\"bindingdb_patent\")\n",
        "\n",
        "        train_val, test = (\n",
        "            dg_benchmark[\"train_val\"],\n",
        "            dg_benchmark[\"test\"],\n",
        "        )\n",
        "\n",
        "        all_drugs = pd.concat([train_val, test])[self._drug_column].unique()\n",
        "        all_targets = pd.concat([train_val, test])[self._target_column].unique()\n",
        "\n",
        "        if self.drug_featurizer.path.exists() and self.target_featurizer.path.exists():\n",
        "            logg.warning(\"Drug and target featurizers already exist\")\n",
        "            return\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        if not self.drug_featurizer.path.exists():\n",
        "            self.drug_featurizer.write_to_disk(all_drugs)\n",
        "\n",
        "        if not self.target_featurizer.path.exists():\n",
        "            self.target_featurizer.write_to_disk(all_targets)\n",
        "\n",
        "        self.drug_featurizer.cpu()\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "    def setup(self, stage: T.Optional[str] = None):\n",
        "        dg_group = dti_dg_group(path=self._data_dir)\n",
        "        dg_benchmark = dg_group.get(\"bindingdb_patent\")\n",
        "        dg_name = dg_benchmark[\"name\"]\n",
        "\n",
        "        self.df_train, self.df_val = dg_group.get_train_valid_split(\n",
        "            benchmark=dg_name, split_type=\"default\", seed=self._seed\n",
        "        )\n",
        "        self.df_test = dg_benchmark[\"test\"]\n",
        "\n",
        "        self._dataframes = [self.df_train, self.df_val]\n",
        "\n",
        "        all_drugs = pd.concat([i[self._drug_column] for i in self._dataframes]).unique()\n",
        "        all_targets = pd.concat(\n",
        "            [i[self._target_column] for i in self._dataframes]\n",
        "        ).unique()\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        self.drug_featurizer.preload(all_drugs)\n",
        "        self.drug_featurizer.cpu()\n",
        "\n",
        "        self.target_featurizer.preload(all_targets)\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.data_train = BinaryDataset(\n",
        "                self.df_train[self._drug_column],\n",
        "                self.df_train[self._target_column],\n",
        "                self.df_train[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "            self.data_val = BinaryDataset(\n",
        "                self.df_val[self._drug_column],\n",
        "                self.df_val[self._target_column],\n",
        "                self.df_val[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.data_test = BinaryDataset(\n",
        "                self.df_test[self._drug_column],\n",
        "                self.df_test[self._target_column],\n",
        "                self.df_test[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.data_train, **self._loader_kwargs)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.data_val, **self._loader_kwargs)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.data_test, **self._loader_kwargs)\n",
        "\n",
        "\n",
        "class EnzPredDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        drug_featurizer: Featurizer,\n",
        "        target_featurizer: Featurizer,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "        seed: int = 0,\n",
        "        batch_size: int = 32,\n",
        "        shuffle: bool = True,\n",
        "        num_workers: int = 0,\n",
        "        header=0,\n",
        "        index_col=0,\n",
        "        sep=\",\",\n",
        "    ):\n",
        "        self._loader_kwargs = {\n",
        "            \"batch_size\": batch_size,\n",
        "            \"shuffle\": shuffle,\n",
        "            \"num_workers\": num_workers,\n",
        "            \"collate_fn\": drug_target_collate_fn,\n",
        "        }\n",
        "\n",
        "        self._csv_kwargs = {\n",
        "            \"header\": header,\n",
        "            \"index_col\": index_col,\n",
        "            \"sep\": sep,\n",
        "        }\n",
        "\n",
        "        self._device = device\n",
        "\n",
        "        self._data_file = Path(data_dir).with_suffix(\".csv\")\n",
        "        self._data_stem = Path(self._data_file.stem)\n",
        "        self._data_dir = self._data_file.parent / self._data_file.stem\n",
        "        self._seed = 0\n",
        "        self._replicate = seed\n",
        "\n",
        "        df = pd.read_csv(self._data_file, index_col=0)\n",
        "        self._drug_column = df.columns[1]\n",
        "        self._target_column = df.columns[0]\n",
        "        self._label_column = df.columns[2]\n",
        "\n",
        "        self.drug_featurizer = drug_featurizer\n",
        "        self.target_featurizer = target_featurizer\n",
        "\n",
        "    @classmethod\n",
        "    def dataset_list(cls):\n",
        "        return [\n",
        "            \"halogenase\",\n",
        "            \"bkace\",\n",
        "            \"gt\",\n",
        "            \"esterase\",\n",
        "            \"kinase\",\n",
        "            \"phosphatase\",\n",
        "        ]\n",
        "\n",
        "    def prepare_data(self):\n",
        "        os.makedirs(self._data_dir, exist_ok=True)\n",
        "\n",
        "        kfsplitter = KFold(n_splits=10, shuffle=True, random_state=self._seed)\n",
        "        full_data = pd.read_csv(self._data_file, index_col=0)\n",
        "\n",
        "        all_drugs = full_data[self._drug_column].unique()\n",
        "        all_targets = full_data[self._target_column].unique()\n",
        "\n",
        "        if self.drug_featurizer.path.exists() and self.target_featurizer.path.exists():\n",
        "            logg.warning(\"Drug and target featurizers already exist\")\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        if not self.drug_featurizer.path.exists():\n",
        "            self.drug_featurizer.write_to_disk(all_drugs)\n",
        "\n",
        "        if not self.target_featurizer.path.exists():\n",
        "            self.target_featurizer.write_to_disk(all_targets)\n",
        "\n",
        "        self.drug_featurizer.cpu()\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "        for i, split in enumerate(kfsplitter.split(full_data)):\n",
        "            fold_train = full_data.iloc[split[0]].reset_index(drop=True)\n",
        "            fold_test = full_data.iloc[split[1]].reset_index(drop=True)\n",
        "            logg.debug(self._data_dir / self._data_stem.with_suffix(f\".{i}.train.csv\"))\n",
        "            fold_train.to_csv(\n",
        "                self._data_dir / self._data_stem.with_suffix(f\".{i}.train.csv\"),\n",
        "                index=True,\n",
        "                header=True,\n",
        "            )\n",
        "            fold_test.to_csv(\n",
        "                self._data_dir / self._data_stem.with_suffix(f\".{i}.test.csv\"),\n",
        "                index=True,\n",
        "                header=True,\n",
        "            )\n",
        "\n",
        "    def setup(self, stage: T.Optional[str] = None):\n",
        "        df_train = pd.read_csv(\n",
        "            self._data_dir\n",
        "            / self._data_stem.with_suffix(f\".{self._replicate}.train.csv\"),\n",
        "            index_col=0,\n",
        "        )\n",
        "        self.df_train, self.df_val = train_test_split(df_train, test_size=0.1)\n",
        "        self.df_test = pd.read_csv(\n",
        "            self._data_dir\n",
        "            / self._data_stem.with_suffix(f\".{self._replicate}.test.csv\"),\n",
        "            index_col=0,\n",
        "        )\n",
        "\n",
        "        self._dataframes = [self.df_train, self.df_val, self.df_test]\n",
        "\n",
        "        all_drugs = pd.concat([i[self._drug_column] for i in self._dataframes]).unique()\n",
        "        all_targets = pd.concat(\n",
        "            [i[self._target_column] for i in self._dataframes]\n",
        "        ).unique()\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        self.drug_featurizer.preload(all_drugs)\n",
        "        self.drug_featurizer.cpu()\n",
        "\n",
        "        self.target_featurizer.preload(all_targets)\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.data_train = BinaryDataset(\n",
        "                self.df_train[self._drug_column],\n",
        "                self.df_train[self._target_column],\n",
        "                self.df_train[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "            self.data_val = BinaryDataset(\n",
        "                self.df_val[self._drug_column],\n",
        "                self.df_val[self._target_column],\n",
        "                self.df_val[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.data_test = BinaryDataset(\n",
        "                self.df_test[self._drug_column],\n",
        "                self.df_test[self._target_column],\n",
        "                self.df_test[self._label_column],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.data_train, **self._loader_kwargs)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.data_val, **self._loader_kwargs)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.data_test, **self._loader_kwargs)\n",
        "\n",
        "\n",
        "class DUDEDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_dir: str,\n",
        "        contrastive_split: str,\n",
        "        drug_featurizer: Featurizer,\n",
        "        target_featurizer: Featurizer,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "        n_neg_per: int = 50,\n",
        "        batch_size: int = 32,\n",
        "        shuffle: bool = True,\n",
        "        num_workers: int = 0,\n",
        "        header=0,\n",
        "        index_col=None,\n",
        "        sep=\"\\t\",\n",
        "    ):\n",
        "        self._loader_kwargs = {\n",
        "            \"batch_size\": batch_size,\n",
        "            \"shuffle\": shuffle,\n",
        "            \"num_workers\": num_workers,\n",
        "            \"collate_fn\": contrastive_collate_fn,\n",
        "        }\n",
        "\n",
        "        self._csv_kwargs = {\n",
        "            \"header\": header,\n",
        "            \"index_col\": index_col,\n",
        "            \"sep\": sep,\n",
        "        }\n",
        "\n",
        "        self._device = device\n",
        "        self._n_neg_per = n_neg_per\n",
        "\n",
        "        self._data_dir = task_dir\n",
        "        self._split = contrastive_split\n",
        "        self._split_path = self._data_dir / Path(\n",
        "            f\"dude_{self._split}_type_train_test_split.csv\"\n",
        "        )\n",
        "\n",
        "        self._drug_id_column = \"Molecule_ID\"\n",
        "        self._drug_column = \"Molecule_SMILES\"\n",
        "        self._target_id_column = \"Target_ID\"\n",
        "        self._target_column = \"Target_Seq\"\n",
        "        self._label_column = \"Label\"\n",
        "\n",
        "        self.drug_featurizer = drug_featurizer\n",
        "        self.target_featurizer = target_featurizer\n",
        "\n",
        "    def prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage: T.Optional[str] = None):\n",
        "        self.df_full = pd.read_csv(\n",
        "            self._data_dir / Path(\"full.tsv\"), **self._csv_kwargs\n",
        "        )\n",
        "\n",
        "        self.df_splits = pd.read_csv(self._split_path, header=None)\n",
        "        self._train_list = self.df_splits[self.df_splits[1] == \"train\"][0].values\n",
        "        self._test_list = self.df_splits[self.df_splits[1] == \"test\"][0].values\n",
        "\n",
        "        self.df_train = self.df_full[\n",
        "            self.df_full[self._target_id_column].isin(self._train_list)\n",
        "        ]\n",
        "        self.df_test = self.df_full[\n",
        "            self.df_full[self._target_id_column].isin(self._test_list)\n",
        "        ]\n",
        "\n",
        "        self.train_contrastive = make_contrastive(\n",
        "            self.df_train,\n",
        "            self._drug_column,\n",
        "            self._target_column,\n",
        "            self._label_column,\n",
        "            self._n_neg_per,\n",
        "        )\n",
        "\n",
        "        self._dataframes = [self.df_train]  # , self.df_test]\n",
        "\n",
        "        all_drugs = pd.concat([i[self._drug_column] for i in self._dataframes]).unique()\n",
        "        all_targets = pd.concat(\n",
        "            [i[self._target_column] for i in self._dataframes]\n",
        "        ).unique()\n",
        "\n",
        "        if self._device.type == \"cuda\":\n",
        "            self.drug_featurizer.cuda(self._device)\n",
        "            self.target_featurizer.cuda(self._device)\n",
        "\n",
        "        self.drug_featurizer.preload(all_drugs, write_first=True)\n",
        "        self.drug_featurizer.cpu()\n",
        "\n",
        "        self.target_featurizer.preload(all_targets, write_first=True)\n",
        "        self.target_featurizer.cpu()\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.data_train = ContrastiveDataset(\n",
        "                self.train_contrastive[\"Anchor\"],\n",
        "                self.train_contrastive[\"Positive\"],\n",
        "                self.train_contrastive[\"Negative\"],\n",
        "                self.drug_featurizer,\n",
        "                self.target_featurizer,\n",
        "            )\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.data_train, **self._loader_kwargs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-b_I4z7G8K0L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LogisticActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Generalized Sigmoid\n",
        "    Applies the element-wise function:\n",
        "    :math:`\\\\sigma(x) = \\\\frac{1}{1 + \\\\exp(-k(x-x_0))}`\n",
        "    :param x0: The value of the sigmoid midpoint\n",
        "    :type x0: float\n",
        "    :param k: The slope of the sigmoid - trainable -  :math:`k \\\\geq 0`\n",
        "    :type k: float\n",
        "    :param train: Whether :math:`k` is a trainable parameter\n",
        "    :type train: bool\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x0=0, k=1, train=False):\n",
        "        super(LogisticActivation, self).__init__()\n",
        "        self.x0 = x0\n",
        "        self.k = nn.Parameter(torch.FloatTensor([float(k)]), requires_grad=False)\n",
        "        self.k.requiresGrad = train\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the function to the input elementwise\n",
        "        :param x: :math:`(N \\\\times *)` where :math:`*` means, any number of additional dimensions\n",
        "        :type x: torch.Tensor\n",
        "        :return: :math:`(N \\\\times *)`, same shape as the input\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        o = torch.clamp(\n",
        "            1 / (1 + torch.exp(-self.k * (x - self.x0))), min=0, max=1\n",
        "        ).squeeze()\n",
        "        return o\n",
        "\n",
        "    def clip(self):\n",
        "        \"\"\"\n",
        "        Restricts sigmoid slope :math:`k` to be greater than or equal to 0, if :math:`k` is trained.\n",
        "        :meta private:\n",
        "        \"\"\"\n",
        "        self.k.data.clamp_(min=0)\n",
        "\n",
        "\n",
        "#######################\n",
        "# Model Architectures #\n",
        "#######################\n",
        "\n",
        "\n",
        "class SimpleCoembedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=\"ReLU\",\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "        self.latent_activation = ACTIVATIONS[latent_activation]\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), self.latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), self.latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        return inner_prod.squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "\n",
        "SimpleCoembeddingNoSigmoid = SimpleCoembedding\n",
        "\n",
        "\n",
        "class SimpleCoembeddingSigmoid(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        sigmoid_f = torch.nn.Sigmoid()\n",
        "        return sigmoid_f(distance).squeeze()\n",
        "\n",
        "\n",
        "class SimpleCoembedding_FoldSeek(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "        foldseek_embedding_dimension=1024,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.foldseek_embedding_dimension = foldseek_embedding_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.foldseek_index_embedding = nn.Embedding(\n",
        "            22,\n",
        "            self.foldseek_embedding_dimension,\n",
        "            padding_idx=FOLDSEEK_MISSING_IDX,\n",
        "        )\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self._target_projector = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                (self.target_shape + self.foldseek_embedding_dimension),\n",
        "                latent_dimension,\n",
        "            ),\n",
        "            latent_activation(),\n",
        "        )\n",
        "        nn.init.xavier_normal_(self._target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def _split_foldseek_target_embedding(self, target_embedding):\n",
        "        \"\"\"\n",
        "        Expect that first dimension of target_embedding is batch dimension, second dimension is [target_shape | protein_length]\n",
        "\n",
        "        FS indexes from 1-21, 0 is padding\n",
        "        target is D + N_pool\n",
        "            first D is PLM embedding\n",
        "            next N_pool is FS index + pool\n",
        "            nn.Embedding ignores elements with padding_idx = 0\n",
        "\n",
        "            N --embedding--> N x D_fs --mean pool--> D_fs\n",
        "            target is (D | D_fs) --linear--> latent\n",
        "        \"\"\"\n",
        "        if target_embedding.shape[1] == self.target_shape:\n",
        "            return target_embedding\n",
        "\n",
        "        plm_embedding = target_embedding[:, : self.target_shape]\n",
        "        foldseek_indices = target_embedding[:, self.target_shape :].long()\n",
        "        foldseek_embedding = self.foldseek_index_embedding(foldseek_indices).mean(dim=1)\n",
        "\n",
        "        full_target_embedding = torch.cat([plm_embedding, foldseek_embedding], dim=1)\n",
        "        return full_target_embedding\n",
        "\n",
        "    def target_projector(self, target):\n",
        "        target_fs_emb = self._split_foldseek_target_embedding(target)\n",
        "        target_projection = self._target_projector(target_fs_emb)\n",
        "        return target_projection\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_fs_emb = self._split_foldseek_target_embedding(target)\n",
        "        target_projection = self._target_projector(target_fs_emb)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "\n",
        "class SimpleCoembedding_FoldSeekX(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "        foldseek_embedding_dimension=512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.foldseek_embedding_dimension = foldseek_embedding_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.foldseek_index_embedding = nn.Embedding(\n",
        "            22,\n",
        "            self.foldseek_embedding_dimension,\n",
        "            padding_idx=FOLDSEEK_MISSING_IDX,\n",
        "        )\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self._target_projector = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                (self.target_shape + self.foldseek_embedding_dimension),\n",
        "                latent_dimension,\n",
        "            ),\n",
        "            latent_activation(),\n",
        "        )\n",
        "        nn.init.xavier_normal_(self._target_projector[0].weight)\n",
        "\n",
        "        # self.projector_dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def _split_foldseek_target_embedding(self, target_embedding):\n",
        "        \"\"\"\n",
        "        Expect that first dimension of target_embedding is batch dimension, second dimension is [target_shape | protein_length]\n",
        "\n",
        "        FS indexes from 1-21, 0 is padding\n",
        "        target is D + N_pool\n",
        "            first D is PLM embedding\n",
        "            next N_pool is FS index + pool\n",
        "            nn.Embedding ignores elements with padding_idx = 0\n",
        "\n",
        "            N --embedding--> N x D_fs --mean pool--> D_fs\n",
        "            target is (D | D_fs) --linear--> latent\n",
        "        \"\"\"\n",
        "        if target_embedding.shape[1] == self.target_shape:\n",
        "            return target_embedding\n",
        "\n",
        "        plm_embedding = target_embedding[:, : self.target_shape]\n",
        "        foldseek_indices = target_embedding[:, self.target_shape :].long()\n",
        "        foldseek_embedding = self.foldseek_index_embedding(foldseek_indices).mean(dim=1)\n",
        "\n",
        "        full_target_embedding = torch.cat([plm_embedding, foldseek_embedding], dim=1)\n",
        "        return full_target_embedding\n",
        "\n",
        "    def target_projector(self, target):\n",
        "        target_fs_emb = self._split_foldseek_target_embedding(target)\n",
        "        target_projection = self._target_projector(target_fs_emb)\n",
        "        return target_projection\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_fs_emb = self._split_foldseek_target_embedding(target)\n",
        "        target_projection = self._target_projector(target_fs_emb)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "\n",
        "class GoldmanCPI(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=100,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        model_dropout=0.2,\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        self.last_layers = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_dimension, latent_dimension, bias=True),\n",
        "            nn.Dropout(p=model_dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_dimension, latent_dimension, bias=True),\n",
        "            nn.Dropout(p=model_dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_dimension, 1, bias=True),\n",
        "        )\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "        output = torch.einsum(\"bd,bd->bd\", drug_projection, target_projection)\n",
        "        distance = self.last_layers(output)\n",
        "        return distance\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        distance = self.regress(drug, target)\n",
        "        sigmoid_f = torch.nn.Sigmoid()\n",
        "        return sigmoid_f(distance).squeeze()\n",
        "\n",
        "\n",
        "class SimpleCosine(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size=2048,\n",
        "        prot_emb_size=100,\n",
        "        latent_size=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        distance_metric=\"Cosine\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(self.prot_emb_size, latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.dist_metric = distance_metric\n",
        "        self.activator = DISTANCE_METRICS[self.dist_metric]()\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_projector(mol_emb)\n",
        "        prot_proj = self.prot_projector(prot_emb)\n",
        "\n",
        "        return self.activator(mol_proj, prot_proj)\n",
        "\n",
        "\n",
        "class AffinityCoembedInner(nn.Module):\n",
        "    def __init__(\n",
        "        self, mol_emb_size, prot_emb_size, latent_size=1024, activation=nn.ReLU\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, latent_size), activation()\n",
        "        )\n",
        "        nn.init.xavier_uniform(self.mol_projector[0].weight)\n",
        "\n",
        "        print(self.mol_projector[0].weight)\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(self.prot_emb_size, latent_size), activation()\n",
        "        )\n",
        "        nn.init.xavier_uniform(self.prot_projector[0].weight)\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_projector(mol_emb)\n",
        "        prot_proj = self.prot_projector(prot_emb)\n",
        "        print(mol_proj)\n",
        "        print(prot_proj)\n",
        "        y = torch.bmm(\n",
        "            mol_proj.view(-1, 1, self.latent_size),\n",
        "            prot_proj.view(-1, self.latent_size, 1),\n",
        "        ).squeeze()\n",
        "        return y\n",
        "\n",
        "\n",
        "class CosineBatchNorm(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size=2048,\n",
        "        prot_emb_size=100,\n",
        "        latent_size=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        distance_metric=\"Cosine\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, self.latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(self.prot_emb_size, self.latent_size),\n",
        "            latent_activation(),\n",
        "        )\n",
        "\n",
        "        self.mol_norm = nn.BatchNorm1d(self.latent_size)\n",
        "        self.prot_norm = nn.BatchNorm1d(self.latent_size)\n",
        "\n",
        "        self.dist_metric = distance_metric\n",
        "        self.activator = DISTANCE_METRICS[self.dist_metric]()\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_norm(self.mol_projector(mol_emb))\n",
        "        prot_proj = self.prot_norm(self.prot_projector(prot_emb))\n",
        "\n",
        "        return self.activator(mol_proj, prot_proj)\n",
        "\n",
        "\n",
        "class LSTMCosine(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size=2048,\n",
        "        prot_emb_size=100,\n",
        "        lstm_layers=3,\n",
        "        lstm_dim=256,\n",
        "        latent_size=256,\n",
        "        latent_activation=nn.ReLU,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            self.prot_emb_size,\n",
        "            lstm_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(2 * lstm_layers * lstm_dim, latent_size), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.activator = nn.CosineSimilarity()\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_projector(mol_emb)\n",
        "\n",
        "        outp, (h_out, _) = self.rnn(prot_emb)\n",
        "        prot_hidden = h_out.permute(1, 0, 2).reshape(outp.shape[0], -1)\n",
        "        prot_proj = self.prot_projector(prot_hidden)\n",
        "\n",
        "        return self.activator(mol_proj, prot_proj)\n",
        "\n",
        "\n",
        "class DeepCosine(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size=2048,\n",
        "        prot_emb_size=100,\n",
        "        latent_size=1024,\n",
        "        hidden_size=4096,\n",
        "        latent_activation=nn.ReLU,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(self.prot_emb_size, hidden_size),\n",
        "            torch.nn.Dropout(p=0.5, inplace=False),\n",
        "            latent_activation(),\n",
        "            nn.Linear(hidden_size, latent_size),\n",
        "            torch.nn.Dropout(p=0.5, inplace=False),\n",
        "            latent_activation(),\n",
        "        )\n",
        "\n",
        "        self.activator = nn.CosineSimilarity()\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_projector(mol_emb)\n",
        "        prot_proj = self.prot_projector(prot_emb)\n",
        "\n",
        "        return self.activator(mol_proj, prot_proj)\n",
        "\n",
        "\n",
        "class SimpleConcat(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size=2048,\n",
        "        prot_emb_size=100,\n",
        "        hidden_dim_1=512,\n",
        "        hidden_dim_2=256,\n",
        "        activation=nn.ReLU,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(mol_emb_size + prot_emb_size, hidden_dim_1), activation()\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(nn.Linear(hidden_dim_1, hidden_dim_2), activation())\n",
        "        self.fc3 = nn.Sequential(nn.Linear(hidden_dim_2, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        cat_emb = torch.cat([mol_emb, prot_emb], axis=1)\n",
        "        return self.fc3(self.fc2(self.fc1(cat_emb))).squeeze()\n",
        "\n",
        "\n",
        "class SeparateConcat(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size=2048,\n",
        "        prot_emb_size=100,\n",
        "        latent_size=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        distance_metric=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(self.prot_emb_size, latent_size), latent_activation()\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(2 * latent_size, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_projector(mol_emb)\n",
        "        prot_proj = self.prot_projector(prot_emb)\n",
        "        cat_emb = torch.cat([mol_proj, prot_proj], axis=1)\n",
        "        return self.fc(cat_emb).squeeze()\n",
        "\n",
        "\n",
        "class AffinityEmbedConcat(nn.Module):\n",
        "    def __init__(\n",
        "        self, mol_emb_size, prot_emb_size, latent_size=1024, activation=nn.ReLU\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.mol_projector = nn.Sequential(\n",
        "            nn.Linear(self.mol_emb_size, latent_size), activation()\n",
        "        )\n",
        "\n",
        "        self.prot_projector = nn.Sequential(\n",
        "            nn.Linear(self.prot_emb_size, latent_size), activation()\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(2 * latent_size, 1)\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        mol_proj = self.mol_projector(mol_emb)\n",
        "        prot_proj = self.prot_projector(prot_emb)\n",
        "        cat_emb = torch.cat([mol_proj, prot_proj], axis=1)\n",
        "        return self.fc(cat_emb).squeeze()\n",
        "\n",
        "\n",
        "SimplePLMModel = AffinityEmbedConcat\n",
        "\n",
        "\n",
        "class AffinityConcatLinear(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mol_emb_size,\n",
        "        prot_emb_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mol_emb_size = mol_emb_size\n",
        "        self.prot_emb_size = prot_emb_size\n",
        "        self.fc = nn.Linear(mol_emb_size + prot_emb_size, 1)\n",
        "\n",
        "    def forward(self, mol_emb, prot_emb):\n",
        "        cat_emb = torch.cat([mol_emb, prot_emb], axis=1)\n",
        "        return self.fc(cat_emb).squeeze()\n",
        "\n"
      ],
      "metadata": {
        "id": "8vxjBtkzxw96"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall rdkit\n",
        "\n",
        "# !pip install rdkit==2021 --quiet\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0L0ChY1Mz6jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_config = '/content/gdrive/MyDrive/test_config.yaml'\n",
        "\n",
        "config = OmegaConf.load(test_config)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(f'cuda:{config.device}' if use_cuda else \"cpu\")\n",
        "print('use_cuda: ', use_cuda)\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIBXWZ_v0Dfc",
        "outputId": "989afece-d9a1-4221-bc2c-3445129c738b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use_cuda:  True\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config.classify = False\n",
        "config.latent_activation = \"GELU\"\n",
        "config.watch_metric = \"val/pcc\"\n"
      ],
      "metadata": {
        "id": "vkvLEmugL8HT"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tdc_data = '/content/gdrive/MyDrive/A_JAK_design/ConPLex_dev/dataset/TDC/dti_dg_group.zip'"
      ],
      "metadata": {
        "id": "Ix7dhhHPB2V-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/A_JAK_design/ConPLex_dev/dataset/TDC/dti_dg_group.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsf_kUm9CC7k",
        "outputId": "15423caa-8256-4d59-eaa2-def05dc5d594"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/A_JAK_design/ConPLex_dev/dataset/TDC/dti_dg_group.zip\n",
            "   creating: dti_dg_group/\n",
            "  inflating: dti_dg_group/.DS_Store  \n",
            "  inflating: __MACOSX/dti_dg_group/._.DS_Store  \n",
            "   creating: dti_dg_group/.ipynb_checkpoints/\n",
            "   creating: dti_dg_group/bindingdb_patent/\n",
            "  inflating: dti_dg_group/bindingdb_patent/.DS_Store  \n",
            "  inflating: __MACOSX/dti_dg_group/bindingdb_patent/._.DS_Store  \n",
            "  inflating: dti_dg_group/bindingdb_patent/test.csv  \n",
            "  inflating: dti_dg_group/bindingdb_patent/train_val.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dti_train = '/content/gdrive/MyDrive/A_JAK_design/dti_dg_group/bindingdb_patent/train_val.csv'\n",
        "dti_train = pd.read_csv(dti_train)\n",
        "print(dti_train.shape) #  (183430, 6)\n",
        "dti_train.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "A27DX9tbCjID",
        "outputId": "58ec303a-9448-49dd-df36-36ed86f36b55"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Drug_ID                                               Drug Target_ID  \\\n",
              "0  91808352.0  COCc1ccccc1C1C(C(=O)C(C)C)C(=O)C(=O)N1c1ccc(cc...    P56373   \n",
              "1  67223437.0  Cc1c2[C@@H]3CCCN([C@@H]3Cc2ccc1C#N)C(=O)c1ccc2...    P28845   \n",
              "2  46222354.0  CC(C)(C#N)c1cccc(C(=O)Nc2cccc(Oc3ccc4nc(NC(=O)...    Q02750   \n",
              "3  59454472.0  CC(=O)Nc1nc2ccc(Oc3cccc(NC(=O)Nc4ccc(cc4)C(C)(...    Q02750   \n",
              "4  46222580.0  Fc1ccc(Oc2ccc3nc(NC(=O)C4CC4)sc3c2C#N)cc1NC(=O...    Q02750   \n",
              "\n",
              "                                              Target         Y  Year  \n",
              "0  MNCISDFFTYETTKSVVVKSWTIGIINRVVQLLIISYFVGWVFLHE...  2.564949  2013  \n",
              "1  MAFMKKYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGI...  4.605170  2013  \n",
              "2  MPKKKPTPIQLNPAPDGSAVNGTSSAETNLEALQKKLEELELDEQQ...  5.703782  2013  \n",
              "3  MPKKKPTPIQLNPAPDGSAVNGTSSAETNLEALQKKLEELELDEQQ...  5.703782  2013  \n",
              "4  MPKKKPTPIQLNPAPDGSAVNGTSSAETNLEALQKKLEELELDEQQ...  5.703782  2013  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-fa39d550-ddf3-4367-b48c-7c99740830dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Drug_ID</th>\n",
              "      <th>Drug</th>\n",
              "      <th>Target_ID</th>\n",
              "      <th>Target</th>\n",
              "      <th>Y</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>91808352.0</td>\n",
              "      <td>COCc1ccccc1C1C(C(=O)C(C)C)C(=O)C(=O)N1c1ccc(cc...</td>\n",
              "      <td>P56373</td>\n",
              "      <td>MNCISDFFTYETTKSVVVKSWTIGIINRVVQLLIISYFVGWVFLHE...</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67223437.0</td>\n",
              "      <td>Cc1c2[C@@H]3CCCN([C@@H]3Cc2ccc1C#N)C(=O)c1ccc2...</td>\n",
              "      <td>P28845</td>\n",
              "      <td>MAFMKKYLLPILGLFMAYYYYSANEEFRPEMLQGKKVIVTGASKGI...</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>46222354.0</td>\n",
              "      <td>CC(C)(C#N)c1cccc(C(=O)Nc2cccc(Oc3ccc4nc(NC(=O)...</td>\n",
              "      <td>Q02750</td>\n",
              "      <td>MPKKKPTPIQLNPAPDGSAVNGTSSAETNLEALQKKLEELELDEQQ...</td>\n",
              "      <td>5.703782</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59454472.0</td>\n",
              "      <td>CC(=O)Nc1nc2ccc(Oc3cccc(NC(=O)Nc4ccc(cc4)C(C)(...</td>\n",
              "      <td>Q02750</td>\n",
              "      <td>MPKKKPTPIQLNPAPDGSAVNGTSSAETNLEALQKKLEELELDEQQ...</td>\n",
              "      <td>5.703782</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>46222580.0</td>\n",
              "      <td>Fc1ccc(Oc2ccc3nc(NC(=O)C4CC4)sc3c2C#N)cc1NC(=O...</td>\n",
              "      <td>Q02750</td>\n",
              "      <td>MPKKKPTPIQLNPAPDGSAVNGTSSAETNLEALQKKLEELELDEQQ...</td>\n",
              "      <td>5.703782</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa39d550-ddf3-4367-b48c-7c99740830dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-677276a5-065d-4192-8ad1-318c645770e0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-677276a5-065d-4192-8ad1-318c645770e0')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-677276a5-065d-4192-8ad1-318c645770e0 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa39d550-ddf3-4367-b48c-7c99740830dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa39d550-ddf3-4367-b48c-7c99740830dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import annotations\n",
        "\n",
        "import typing as T\n",
        "\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "# from src.featurizers.molecule import MorganFeaturizer\n",
        "\n",
        "# import MorganFeaturizer, ProtBertFeaturizer\n",
        "# from model.architectures import SimpleCoembeddingNoSigmoid\n",
        "\n",
        "class Featurizer:\n",
        "    def __init__(\n",
        "        self, name: str, shape: int, save_dir: Path = Path().absolute()\n",
        "    ):\n",
        "        self._name = name\n",
        "        self._shape = shape\n",
        "        self._save_path = save_dir / Path(f\"{self._name}_features.h5\")\n",
        "\n",
        "        self._preloaded = False\n",
        "        self._device = torch.device(\"cpu\")\n",
        "        self._cuda_registry = {}\n",
        "        self._on_cuda = False\n",
        "        self._features = {}\n",
        "\n",
        "    def __call__(self, seq: str) -> torch.Tensor:\n",
        "        if seq not in self.features:\n",
        "            self._features[seq] = self.transform(seq)\n",
        "\n",
        "        return self._features[seq]\n",
        "\n",
        "    def _register_cuda(self, k: str, v, f=None):\n",
        "        \"\"\"\n",
        "        Register an object as capable of being moved to a CUDA device\n",
        "        \"\"\"\n",
        "        self._cuda_registry[k] = (v, f)\n",
        "\n",
        "    def _transform(self, seq: str) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _update_device(self, device: torch.device):\n",
        "        self._device = device\n",
        "        for k, (v, f) in self._cuda_registry.items():\n",
        "            if f is None:\n",
        "                try:\n",
        "                    self._cuda_registry[k] = (v.to(self._device), None)\n",
        "                except RuntimeError as e:\n",
        "                    logg.error(e)\n",
        "                    logg.debug(device)\n",
        "                    logg.debug(type(self._device))\n",
        "                    logg.debug(self._device)\n",
        "            else:\n",
        "                self._cuda_registry[k] = (f(v, self._device), f)\n",
        "        for k, v in self._features.items():\n",
        "            self._features[k] = v.to(device)\n",
        "\n",
        "    @lru_cache(maxsize=5000)\n",
        "    def transform(self, seq: str) -> torch.Tensor:\n",
        "        with torch.set_grad_enabled(False):\n",
        "            feats = self._transform(seq)\n",
        "            if self._on_cuda:\n",
        "                feats = feats.to(self.device)\n",
        "            return feats\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return self._name\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> int:\n",
        "        return self._shape\n",
        "\n",
        "    @property\n",
        "    def path(self) -> Path:\n",
        "        return self._save_path\n",
        "\n",
        "    @property\n",
        "    def features(self) -> dict:\n",
        "        return self._features\n",
        "\n",
        "    @property\n",
        "    def on_cuda(self) -> bool:\n",
        "        return self._on_cuda\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return self._device\n",
        "\n",
        "    def to(self, device: torch.device) -> Featurizer:\n",
        "        self._update_device(device)\n",
        "        self._on_cuda = device.type == \"cuda\"\n",
        "        return self\n",
        "\n",
        "    def cuda(self, device: torch.device) -> Featurizer:\n",
        "        \"\"\"\n",
        "        Perform model computations on CUDA, move saved embeddings to CUDA device\n",
        "        \"\"\"\n",
        "        self._update_device(device)\n",
        "        self._on_cuda = True\n",
        "        return self\n",
        "\n",
        "    def cpu(self) -> Featurizer:\n",
        "        \"\"\"\n",
        "        Perform model computations on CPU, move saved embeddings to CPU\n",
        "        \"\"\"\n",
        "        self._update_device(torch.device(\"cpu\"))\n",
        "        self._on_cuda = False\n",
        "        return self\n",
        "\n",
        "    def write_to_disk(\n",
        "        self, seq_list: T.List[str], verbose: bool = True\n",
        "    ) -> None:\n",
        "        logg.info(f\"Writing {self.name} features to {self.path}\")\n",
        "        with h5py.File(self._save_path, \"a\") as h5fi:\n",
        "            for seq in tqdm(seq_list, disable=not verbose, desc=self.name):\n",
        "                seq_h5 = sanitize_string(seq)\n",
        "                if seq_h5 in h5fi:\n",
        "                    logg.warning(f\"{seq} already in h5file\")\n",
        "                feats = self.transform(seq)\n",
        "                dset = h5fi.require_dataset(seq_h5, feats.shape, np.float32)\n",
        "                dset[:] = feats.cpu().numpy()\n",
        "\n",
        "    def preload(\n",
        "        self,\n",
        "        seq_list: T.List[str],\n",
        "        verbose: bool = True,\n",
        "        write_first: bool = True,\n",
        "    ) -> None:\n",
        "        logg.info(f\"Preloading {self.name} features from {self.path}\")\n",
        "\n",
        "        if write_first and not self._save_path.exists():\n",
        "            self.write_to_disk(seq_list, verbose=verbose)\n",
        "\n",
        "        if self._save_path.exists():\n",
        "            with h5py.File(self._save_path, \"r\") as h5fi:\n",
        "                for seq in tqdm(seq_list, disable=not verbose, desc=self.name):\n",
        "                    if seq in h5fi:\n",
        "                        seq_h5 = sanitize_string(seq)\n",
        "                        feats = torch.from_numpy(h5fi[seq_h5][:])\n",
        "                    else:\n",
        "                        feats = self.transform(seq)\n",
        "\n",
        "                    if self._on_cuda:\n",
        "                        feats = feats.to(self.device)\n",
        "\n",
        "                    self._features[seq] = feats\n",
        "\n",
        "        else:\n",
        "            for seq in tqdm(seq_list, disable=not verbose, desc=self.name):\n",
        "                feats = self.transform(seq)\n",
        "\n",
        "                if self._on_cuda:\n",
        "                    feats = feats.to(self.device)\n",
        "\n",
        "                self._features[seq] = feats\n",
        "\n",
        "        # seqs_sanitized = [sanitize_string(s) for s in seq_list]\n",
        "        # feat_dict = load_hdf5_parallel(self._save_path, seqs_sanitized,n_jobs=32)\n",
        "        # self._features.update(feat_dict)\n",
        "\n",
        "        self._update_device(self.device)\n",
        "        self._preloaded = True\n",
        "\n",
        "class MorganFeaturizer(Featurizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        shape: int = 2048,\n",
        "        radius: int = 2,\n",
        "        save_dir: Path = Path().absolute(),\n",
        "    ):\n",
        "        super().__init__(\"Morgan\", shape, save_dir)\n",
        "\n",
        "        self._radius = radius\n",
        "\n",
        "    def smiles_to_morgan(self, smile: str):\n",
        "        \"\"\"\n",
        "        Convert smiles into Morgan Fingerprint.\n",
        "        :param smile: SMILES string\n",
        "        :type smile: str\n",
        "        :return: Morgan fingerprint\n",
        "        :rtype: np.ndarray\n",
        "        \"\"\"\n",
        "        try:\n",
        "            smile = canonicalize(smile)\n",
        "            mol = Chem.MolFromSmiles(smile)\n",
        "            features_vec = AllChem.GetMorganFingerprintAsBitVect(\n",
        "                mol, self._radius, nBits=self.shape\n",
        "            )\n",
        "            features = np.zeros((1,))\n",
        "            DataStructs.ConvertToNumpyArray(features_vec, features)\n",
        "        except Exception as e:\n",
        "            logg.error(\n",
        "                f\"rdkit not found this smiles for morgan: {smile} convert to all 0 features\"\n",
        "            )\n",
        "            logg.error(e)\n",
        "            features = np.zeros((self.shape,))\n",
        "        return features\n",
        "\n",
        "    def _transform(self, smile: str) -> torch.Tensor:\n",
        "        # feats = torch.from_numpy(self._featurizer(smile)).squeeze().float()\n",
        "        feats = (\n",
        "            torch.from_numpy(self.smiles_to_morgan(smile)).squeeze().float()\n",
        "        )\n",
        "        if feats.shape[0] != self.shape:\n",
        "            logg.warning(\"Failed to featurize: appending zero vector\")\n",
        "            feats = torch.zeros(self.shape)\n",
        "        return feats\n",
        "\n",
        "\n",
        "class SimpleCoembedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "class SimpleCoembeddingSigmoid(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=nn.ReLU,\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.drug_projector[0].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension), latent_activation()\n",
        "        )\n",
        "        nn.init.xavier_normal_(self.target_projector[0].weight)\n",
        "\n",
        "        if self.do_classify:\n",
        "            self.distance_metric = latent_distance\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        relu_f = torch.nn.ReLU()\n",
        "        return relu_f(inner_prod).squeeze()\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        sigmoid_f = torch.nn.Sigmoid()\n",
        "        return sigmoid_f(distance).squeeze()\n",
        "from pathlib import Path\n",
        "MODEL_CACHE_DIR = Path(\n",
        "    '/content/gdrive/MyDrive/A_JAK_design/ConPLex_dev/', \"..\", \"..\", \"models\")\n",
        "\n",
        "class ProtBertFeaturizer(Featurizer):\n",
        "    def __init__(self, save_dir: Path = Path().absolute(), per_tok=False):\n",
        "        super().__init__(\"ProtBert\", 1024, save_dir)\n",
        "\n",
        "        from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "\n",
        "        self._max_len = 1024\n",
        "        self.per_tok = per_tok\n",
        "\n",
        "        self._protbert_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"Rostlab/prot_bert\",\n",
        "            do_lower_case=False,\n",
        "            cache_dir=f\"{MODEL_CACHE_DIR}/huggingface/transformers\",\n",
        "        )\n",
        "        self._protbert_model = AutoModel.from_pretrained(\n",
        "            \"Rostlab/prot_bert\",\n",
        "            cache_dir=f\"{MODEL_CACHE_DIR}/huggingface/transformers\",\n",
        "        )\n",
        "        self._protbert_feat = pipeline(\n",
        "            \"feature-extraction\",\n",
        "            model=self._protbert_model,\n",
        "            tokenizer=self._protbert_tokenizer,\n",
        "        )\n",
        "\n",
        "        self._register_cuda(\"model\", self._protbert_model)\n",
        "        self._register_cuda(\n",
        "            \"featurizer\", self._protbert_feat, self._feat_to_device\n",
        "        )\n",
        "\n",
        "    def _feat_to_device(self, pipe, device):\n",
        "        from transformers import pipeline\n",
        "\n",
        "        if device.type == \"cpu\":\n",
        "            d = -1\n",
        "        else:\n",
        "            d = device.index\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"feature-extraction\",\n",
        "            model=self._protbert_model,\n",
        "            tokenizer=self._protbert_tokenizer,\n",
        "            device=d,\n",
        "        )\n",
        "        self._protbert_feat = pipe\n",
        "        return pipe\n",
        "\n",
        "    def _space_sequence(self, x):\n",
        "        return \" \".join(list(x))\n",
        "\n",
        "    def _transform(self, seq: str):\n",
        "        if len(seq) > self._max_len - 2:\n",
        "            seq = seq[: self._max_len - 2]\n",
        "\n",
        "        embedding = torch.tensor(\n",
        "            self._cuda_registry[\"featurizer\"][0](self._space_sequence(seq))\n",
        "        )\n",
        "        seq_len = len(seq)\n",
        "        start_Idx = 1\n",
        "        end_Idx = seq_len + 1\n",
        "        feats = embedding.squeeze()[start_Idx:end_Idx]\n",
        "\n",
        "        if self.per_tok:\n",
        "            return feats\n",
        "        return feats.mean(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAWByi_XC61R"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_dir = 'test_conplex_train/'\n",
        "# create_path(task_dir)"
      ],
      "metadata": {
        "id": "Q2jwsGYMDWRM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logg.debug(f\"Setting random state {config.replicate}\")\n",
        "set_random_seed(config.replicate)\n",
        "logg.info(\"Preparing DataModule\")\n",
        "# task_dir = get_task_dir(config.task, database_root=data_cache_dir)\n",
        "\n",
        "target_featurizer = ProtBertFeaturizer(\n",
        "    save_dir=task_dir, per_tok=False)\n",
        "drug_featurizer = MorganFeaturizer(save_dir=task_dir)"
      ],
      "metadata": {
        "id": "nLJcsYf6BCGg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datamodule = TDCDataModule(\n",
        "    task_dir,\n",
        "    drug_featurizer,\n",
        "    target_featurizer,\n",
        "    device=device,\n",
        "    seed=config.replicate,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=config.shuffle,\n",
        "    num_workers=config.num_workers,\n",
        ")"
      ],
      "metadata": {
        "id": "7raYmXjJEmtc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_string(s):\n",
        "    return s.replace(\"/\", \"|\")\n",
        "datamodule.prepare_data()\n",
        "datamodule.setup()\n",
        "\n",
        "# Load DataLoaders\n",
        "logg.info(\"Getting DataLoaders\")\n",
        "training_generator = datamodule.train_dataloader()\n",
        "validation_generator = datamodule.val_dataloader()\n",
        "testing_generator = datamodule.test_dataloader()"
      ],
      "metadata": {
        "id": "D47jE2ctEz_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.drug_shape = drug_featurizer.shape\n",
        "config.target_shape = target_featurizer.shape"
      ],
      "metadata": {
        "id": "njSy0t6KGdRS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logg.info(\"Initializing model\")\n",
        "model = SimpleCoembeddingNoSigmoid(\n",
        "    config.drug_shape,\n",
        "    config.target_shape,\n",
        "    latent_dimension=config.latent_dimension,\n",
        "    latent_distance=config.latent_distance,\n",
        "    latent_activation=config.latent_activation,\n",
        "    classify=config.classify,\n",
        ")\n",
        "if \"checkpoint\" in config:\n",
        "    state_dict = torch.load(config.checkpoint)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "model = model.to(device)\n",
        "logg.info(model)\n",
        "\n",
        "# Optimizers\n",
        "logg.info(\"Initializing optimizers\")\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    opt, T_0=config.lr_t0\n",
        ")"
      ],
      "metadata": {
        "id": "4hC7vHs_0Ie8"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logg.info(\"Initializing metrics\")\n",
        "max_metric = 0\n",
        "model_max = copy.deepcopy(model)\n",
        "loss_fct = torch.nn.MSELoss()\n",
        "val_metrics = {\n",
        "    \"val/mse\": torchmetrics.MeanSquaredError,\n",
        "    \"val/pcc\": torchmetrics.PearsonCorrCoef,\n",
        "}"
      ],
      "metadata": {
        "id": "dOas_Qdm0TFl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wandb_log(m, do_wandb=True):\n",
        "    if do_wandb:\n",
        "        wandb.log(m)\n"
      ],
      "metadata": {
        "id": "paYHO7seIAZi"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "import json\n",
        "do_wandb = config.wandb_save and (\"wandb_proj\" in config)\n",
        "if do_wandb:\n",
        "    logg.info(f\"Initializing wandb project {config.wandb_proj}\")\n",
        "    wandb.init(\n",
        "        project=config.wandb_proj,\n",
        "        name='test_run',\n",
        "        config=dict(config),\n",
        "    )\n",
        "    wandb.watch(model, log_freq=100)\n",
        "logg.info(\"Config:\")\n",
        "logg.info(json.dumps(dict(config), indent=4))\n",
        "\n",
        "logg.info(\"Beginning Training\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291,
          "referenced_widgets": [
            "08bacd2d38f6427891cf01217e42cb17",
            "cb0cc63dcd384c62a199faf82196ce32",
            "f0429fea9a07473a931e7605f6308826",
            "e223bcbb6f5e4c7d95ca5c69512a2913",
            "bb9cca19f0e242eba0240104347eb433",
            "7c919a8b447d43ea9a4d76fa09e774a9",
            "33cb4dbda4474a89918493cd802c0741",
            "1d07f9b297744cd7894ee2b06002500a"
          ]
        },
        "id": "KITCINghHjBk",
        "outputId": "2b1ef22a-6a60-4ca6-9445-0b37ae175145"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2y3z0wmv) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.144027…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08bacd2d38f6427891cf01217e42cb17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">test_run</strong> at: <a href='https://wandb.ai/lanlang/NoSigmoidTest/runs/2y3z0wmv' target=\"_blank\">https://wandb.ai/lanlang/NoSigmoidTest/runs/2y3z0wmv</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230726_213252-2y3z0wmv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2y3z0wmv). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/A_JAK_design/wandb/run-20230726_213319-027bovwm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lanlang/NoSigmoidTest/runs/027bovwm' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/lanlang/NoSigmoidTest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lanlang/NoSigmoidTest' target=\"_blank\">https://wandb.ai/lanlang/NoSigmoidTest</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lanlang/NoSigmoidTest/runs/027bovwm' target=\"_blank\">https://wandb.ai/lanlang/NoSigmoidTest/runs/027bovwm</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "config.contrastive = False\n",
        "FOLDSEEK_MISSING_IDX = 20\n"
      ],
      "metadata": {
        "id": "nRn_34t0IDNg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def test(model, data_generator, metrics, device=None, classify=True):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    metric_dict = {}\n",
        "\n",
        "    for k, met_class in metrics.items():\n",
        "        if classify:\n",
        "            met_instance = met_class(task=\"binary\")\n",
        "        else:\n",
        "            met_instance = met_class()\n",
        "        met_instance.to(device)\n",
        "        met_instance.reset()\n",
        "        metric_dict[k] = met_instance\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for _, batch in tqdm(enumerate(data_generator), total=len(data_generator)):\n",
        "        pred, label = step(model, batch, device)\n",
        "        if classify:\n",
        "            label = label.int()\n",
        "        else:\n",
        "            label = label.float()\n",
        "\n",
        "        for _, met_instance in metric_dict.items():\n",
        "            met_instance(pred, label)\n",
        "\n",
        "    results = {}\n",
        "    for k, met_instance in metric_dict.items():\n",
        "        res = met_instance.compute()\n",
        "        results[k] = res\n",
        "\n",
        "    for met_instance in metric_dict.values():\n",
        "        met_instance.to(\"cpu\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "WCgmVxRfIy2Y"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in tqdm(enumerate(training_generator),\n",
        "                     total=len(training_generator)):\n",
        "    print(batch)\n"
      ],
      "metadata": {
        "id": "spgTktK-IqH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVYFNMTJPmP9",
        "outputId": "ca4c425e-68e5-45c6-943e-a833b4f4cb53"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JJ2aJ1ePp5g",
        "outputId": "8e842ae9-327a-416a-a855-ed16a7ef2360"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bTFn78fPrZ6",
        "outputId": "1c3be5a7-5ee3-4c14-db77-d30186476106"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8oAhmiZPtFL",
        "outputId": "623c005d-6c70-43ce-89c9-e9a18c1e8b3c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3.2581,  0.8329,  1.9601,  6.9078,  3.4965,  6.0162,  4.6052,  2.8904,\n",
              "         7.3132,  6.4552,  0.8755,  2.9957,  6.2146,  4.6052,  0.6471,  9.2591,\n",
              "         2.0794,  3.8501,  4.1414,  3.6376,  4.6052,  0.4688,  3.3322,  4.8675,\n",
              "         7.1309, 14.8688,  8.7794,  4.4427,  5.2883,  5.6348,  3.4372,  5.8889],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = 'save_conplex/'\n",
        "create_path(save_dir)\n",
        "\n",
        "loss_fct = torch.nn.MSELoss()\n",
        "val_metrics = {\n",
        "    \"val/mse\": torchmetrics.MeanSquaredError,\n",
        "    \"val/pcc\": torchmetrics.PearsonCorrCoef,\n",
        "}\n",
        "\n",
        "test_metrics = {\n",
        "    \"test/mse\": torchmetrics.MeanSquaredError,\n",
        "    \"test/pcc\": torchmetrics.PearsonCorrCoef,\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNUf_dkVKX5h",
        "outputId": "2ff80f94-0939-43c9-fb2e-69926f9b8b0a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save_conplex/  folder is in directory:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def step(model, batch, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    drug, target, label = batch  # target is (D + N_pool)\n",
        "    pred = model(drug.to(device), target.to(device))\n",
        "    label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
        "    return pred, label\n",
        "\n",
        "\n",
        "start_time = time()\n",
        "# for epo in range(config.epochs):\n",
        "\n",
        "for epo in range(1):\n",
        "\n",
        "    model.train()\n",
        "    epoch_time_start = time()\n",
        "\n",
        "    # Main Step\n",
        "    for i, batch in tqdm(\n",
        "        enumerate(training_generator), total=len(training_generator)\n",
        "    ):\n",
        "        pred, label = step(model, batch, device)  # batch is (2048, 1024, 1)\n",
        "\n",
        "        loss = loss_fct(pred, label)\n",
        "\n",
        "        wandb_log(\n",
        "            {\n",
        "                \"train/step\": (epo * len(training_generator) * config.batch_size)\n",
        "                + (i * config.batch_size),\n",
        "                \"train/loss\": loss,\n",
        "            },\n",
        "            do_wandb,\n",
        "        )\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    wandb_log(\n",
        "        {\n",
        "            \"epoch\": epo,\n",
        "            \"train/lr\": lr_scheduler.get_lr()[0],\n",
        "        },\n",
        "        do_wandb,\n",
        "    )\n",
        "    logg.info(\n",
        "        f\"Training at Epoch {epo + 1} with loss {loss.cpu().detach().numpy():8f}\"\n",
        "    )\n",
        "    logg.info(f\"Updating learning rate to {lr_scheduler.get_lr()[0]:8f}\")\n",
        "\n",
        "    # Contrastive Step\n",
        "    if config.contrastive:\n",
        "        logg.info(f\"Training contrastive at Epoch {epo + 1}\")\n",
        "        for i, batch in tqdm(\n",
        "            enumerate(contrastive_generator),\n",
        "            total=len(contrastive_generator),\n",
        "        ):\n",
        "            anchor, positive, negative = contrastive_step(model, batch, device)\n",
        "\n",
        "            contrastive_loss = contrastive_loss_fct(anchor, positive, negative)\n",
        "\n",
        "            wandb_log(\n",
        "                {\n",
        "                    \"train/c_step\": (\n",
        "                        epo\n",
        "                        * len(training_generator)\n",
        "                        * config.contrastive_batch_size\n",
        "                    )\n",
        "                    + (i * config.contrastive_batch_size),\n",
        "                    \"train/c_loss\": contrastive_loss,\n",
        "                },\n",
        "                do_wandb,\n",
        "            )\n",
        "\n",
        "            opt_contrastive.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            opt_contrastive.step()\n",
        "\n",
        "        contrastive_loss_fct.step()\n",
        "        lr_scheduler_contrastive.step()\n",
        "\n",
        "        wandb_log(\n",
        "            {\n",
        "                \"epoch\": epo,\n",
        "                \"train/triplet_margin\": contrastive_loss_fct.margin,\n",
        "                \"train/contrastive_lr\": lr_scheduler_contrastive.get_lr(),\n",
        "            },\n",
        "            do_wandb,\n",
        "        )\n",
        "\n",
        "        logg.info(\n",
        "            f\"Training at Contrastive Epoch {epo + 1} with loss {contrastive_loss.cpu().detach().numpy():8f}\"\n",
        "        )\n",
        "        logg.info(\n",
        "            f\"Updating contrastive learning rate to {lr_scheduler_contrastive.get_lr()[0]:8f}\"\n",
        "        )\n",
        "        logg.info(f\"Updating contrastive margin to {contrastive_loss_fct.margin}\")\n",
        "\n",
        "    epoch_time_end = time()\n",
        "\n",
        "    # Validation\n",
        "    if epo % config.every_n_val == 0:\n",
        "        with torch.set_grad_enabled(False):\n",
        "            val_results = test(\n",
        "                model,\n",
        "                validation_generator,\n",
        "                val_metrics,\n",
        "                device,\n",
        "                config.classify,\n",
        "            )\n",
        "\n",
        "            val_results[\"epoch\"] = epo\n",
        "            val_results[\"Charts/epoch_time\"] = (\n",
        "                epoch_time_end - epoch_time_start\n",
        "            ) / config.every_n_val\n",
        "\n",
        "            wandb_log(val_results, do_wandb)\n",
        "\n",
        "            if val_results[config.watch_metric] > max_metric:\n",
        "                logg.debug(\n",
        "                    f\"Validation AUPR {val_results[config.watch_metric]:8f} > previous max {max_metric:8f}\"\n",
        "                )\n",
        "                model_max = copy.deepcopy(model)\n",
        "                max_metric = val_results[config.watch_metric]\n",
        "                model_save_path = Path(\n",
        "                    f\"{save_dir}/test_best_model_epoch{epo:02}.pt\"\n",
        "                )\n",
        "                torch.save(\n",
        "                    model_max.state_dict(),\n",
        "                    model_save_path,\n",
        "                )\n",
        "                logg.info(f\"Saving checkpoint model to {model_save_path}\")\n",
        "\n",
        "                if do_wandb:\n",
        "                    art = wandb.Artifact(f\"dti-{config.run_id}\", type=\"model\")\n",
        "                    art.add_file(model_save_path, model_save_path.name)\n",
        "                    wandb.log_artifact(art, aliases=[\"best\"])\n",
        "\n",
        "            logg.info(f\"Validation at Epoch {epo + 1}\")\n",
        "            for k, v in val_results.items():\n",
        "                if not k.startswith(\"_\"):\n",
        "                    logg.info(f\"{k}: {v}\")\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Testing\n",
        "logg.info(\"Beginning testing\")\n",
        "try:\n",
        "    with torch.set_grad_enabled(False):\n",
        "        model_max = model_max.eval()\n",
        "\n",
        "        test_start_time = time()\n",
        "        test_results = test(\n",
        "            model_max,\n",
        "            testing_generator,\n",
        "            test_metrics,\n",
        "            device,\n",
        "            config.classify,\n",
        "        )\n",
        "        test_end_time = time()\n",
        "\n",
        "        test_results[\"epoch\"] = epo + 1\n",
        "        test_results[\"test/eval_time\"] = test_end_time - test_start_time\n",
        "        test_results[\"Charts/wall_clock_time\"] = end_time - start_time\n",
        "        wandb_log(test_results, do_wandb)\n",
        "\n",
        "        logg.info(\"Final Testing\")\n",
        "        for k, v in test_results.items():\n",
        "            if not k.startswith(\"_\"):\n",
        "                logg.info(f\"{k}: {v}\")\n",
        "\n",
        "        model_save_path = Path(f\"{save_dir}/{config.run_id}_best_model.pt\")\n",
        "        torch.save(\n",
        "            model_max.state_dict(),\n",
        "            model_save_path,\n",
        "        )\n",
        "        logg.info(f\"Saving final model to {model_save_path}\")\n",
        "\n",
        "        if do_wandb:\n",
        "            art = wandb.Artifact(f\"dti-{config.run_id}\", type=\"model\")\n",
        "            art.add_file(model_save_path, model_save_path.name)\n",
        "            wandb.log_artifact(art, aliases=[\"best\"])\n",
        "\n",
        "except Exception as e:\n",
        "    logg.error(f\"Testing failed with exception {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F_qI66WFH0Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conplex-dti train --run-id TestRun --config config/default_config.yaml\n"
      ],
      "metadata": {
        "id": "bhUIW0Jx1LAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN1cYTXdOLUw",
        "outputId": "c1d10d6d-8f78-418d-c814-f19f8cef1c92"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbins\u001b[0m/            \u001b[01;34mdti_dg_group\u001b[0m/   \u001b[01;34mmodel\u001b[0m/      \u001b[01;34msave_conplex\u001b[0m/         test.csv\n",
            "\u001b[01;34mCoGT\u001b[0m/            \u001b[01;34mFigures\u001b[0m/        \u001b[01;34mmol2vec\u001b[0m/    \u001b[01;34mSELFIES_VAE\u001b[0m/          \u001b[01;34mwandb\u001b[0m/\n",
            "\u001b[01;34mconditionalVAE\u001b[0m/  \u001b[01;34mhelp_function\u001b[0m/  \u001b[01;34mMTATFP\u001b[0m/     \u001b[01;34msun_lab_compound_pk\u001b[0m/\n",
            "\u001b[01;34mConPLex_dev\u001b[0m/     \u001b[01;34m__MACOSX\u001b[0m/       myfile.txt  \u001b[01;34mtest\u001b[0m/\n",
            "\u001b[01;34mData\u001b[0m/            \u001b[01;34mMNIST\u001b[0m/          \u001b[01;34mSA_Score\u001b[0m/   \u001b[01;34mtest_conplex_train\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fMQAnbAIOLoD",
        "outputId": "31e655f7-ba80-492d-cd99-c12126781c94"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'save_conplex/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd save_conplex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnOkpfyuOMwC",
        "outputId": "a0087ae5-ad1e-444f-9a28-557f173ae0c2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/A_JAK_design/save_conplex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85OJtcaEOP5K",
        "outputId": "1909992e-cec1-451d-f6ae-c88dd8e2d0fb"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_best_model_epoch00.pt  test_best_model_epoch02.pt\n",
            "test_best_model_epoch01.pt  test_run_best_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = '/content/gdrive/MyDrive/A_JAK_design/ConPLex_dev/fedratinib_jak2_2lines.tsv'\n",
        "try:\n",
        "    query_df = pd.read_csv(\n",
        "        data_file,\n",
        "        sep=\"\\t\",\n",
        "        names=[\"proteinID\", \"moleculeID\", \"proteinSequence\", \"moleculeSmiles\"],\n",
        "        index_col=False\n",
        "    )\n",
        "except FileNotFoundError:\n",
        "    print(f\"Could not find data file: {data_file}\")\n",
        "# query_df['moleculeSmiles'] = 'Cc1cnc(Nc2ccc(OCCN3CCCC3)cc2)nc1Nc1cccc(S(=O)(=O)NC(C)(C)C)c1'\n",
        "# query_df.to_csv('test_new.tsv', sep=\"\\t\", index=False, header=False)\n",
        "\n",
        "# query_df\n",
        "bar = 'CCS(=O)(=O)N1CC(C1)(CC#N)N2C=C(C=N2)C3=C4C=CNC4=NC=N3'\n",
        "query_df['moleculeSmiles'].loc[1:]= bar\n",
        "query_df\n",
        "# query_df.to_csv('test_new.tsv', sep=\"\\t\", index=False, header=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "uIEAOfmmOQH7",
        "outputId": "42b3ed5a-1a97-44a1-c123-4cbfa8e2530c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         proteinID     moleculeID  \\\n",
              "0  ENSP00000371067  CHEMBL1287853   \n",
              "1  ENSP00000371067  CHEMBL1287853   \n",
              "\n",
              "                                     proteinSequence  \\\n",
              "0  MGMACLTMTEMEGTSTSSIYQNGDISGNANSMKQIDPVLQVYLYHS...   \n",
              "1  MGMACLTMTEMEGTSTSSIYQNGDISGNANSMKQIDPVLQVYLYHS...   \n",
              "\n",
              "                                      moleculeSmiles  \n",
              "0  Cc1cnc(Nc2ccc(OCCN3CCCC3)cc2)nc1Nc1cccc(S(=O)(...  \n",
              "1  CCS(=O)(=O)N1CC(C1)(CC#N)N2C=C(C=N2)C3=C4C=CNC...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9bff61aa-4973-47c5-950d-5f0e1dbf5a9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proteinID</th>\n",
              "      <th>moleculeID</th>\n",
              "      <th>proteinSequence</th>\n",
              "      <th>moleculeSmiles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSP00000371067</td>\n",
              "      <td>CHEMBL1287853</td>\n",
              "      <td>MGMACLTMTEMEGTSTSSIYQNGDISGNANSMKQIDPVLQVYLYHS...</td>\n",
              "      <td>Cc1cnc(Nc2ccc(OCCN3CCCC3)cc2)nc1Nc1cccc(S(=O)(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSP00000371067</td>\n",
              "      <td>CHEMBL1287853</td>\n",
              "      <td>MGMACLTMTEMEGTSTSSIYQNGDISGNANSMKQIDPVLQVYLYHS...</td>\n",
              "      <td>CCS(=O)(=O)N1CC(C1)(CC#N)N2C=C(C=N2)C3=C4C=CNC...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bff61aa-4973-47c5-950d-5f0e1dbf5a9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ea911463-cd66-4b4b-bab2-63cc3336b08f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea911463-cd66-4b4b-bab2-63cc3336b08f')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ea911463-cd66-4b4b-bab2-63cc3336b08f button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9bff61aa-4973-47c5-950d-5f0e1dbf5a9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9bff61aa-4973-47c5-950d-5f0e1dbf5a9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_string(s):\n",
        "    return s.replace(\"/\", \"|\")\n",
        "model_path = 'test_run_best_model.pt'\n",
        "target_featurizer = ProtBertFeaturizer(\n",
        "    save_dir='/', per_tok=False)\n",
        "drug_featurizer = MorganFeaturizer(save_dir='/')\n",
        "drug_featurizer.preload(query_df[\"moleculeSmiles\"].unique())\n",
        "target_featurizer.preload(query_df[\"proteinSequence\"].unique())\n",
        "\n",
        "SimpleCoembeddingNoSigmoid = SimpleCoembedding\n",
        "\n",
        "DISTANCE_METRICS = {\n",
        "    \"Cosine\": Cosine,\n",
        "    \"SquaredCosine\": SquaredCosine,\n",
        "    \"Euclidean\": Euclidean,\n",
        "    \"SquaredEuclidean\": SquaredEuclidean,\n",
        "}\n",
        "\n",
        "ACTIVATIONS = {\"ReLU\": nn.ReLU, \"GELU\": nn.GELU, \"ELU\": nn.ELU, \"Sigmoid\": nn.Sigmoid}\n",
        "\n",
        "model = SimpleCoembeddingNoSigmoid(\n",
        "    drug_featurizer.shape, target_featurizer.shape, 1024\n",
        ")\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model = model.eval()\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs_S_gc-OcNN",
        "outputId": "9df4934f-19d8-427c-8754-c14402582379"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Morgan: 100%|██████████| 2/2 [00:00<00:00, 373.84it/s]\n",
            "Morgan: 100%|██████████| 2/2 [00:00<00:00, 1253.90it/s]\n",
            "ProtBert: 100%|██████████| 1/1 [00:04<00:00,  4.63s/it]\n",
            "ProtBert: 100%|██████████| 1/1 [00:00<00:00, 737.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drug_featurizer.to(device)\n",
        "target_featurizer.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soejxuALO9O_",
        "outputId": "ce3987a7-26f4-4a96-8de8-e6afcb403af3"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.ProtBertFeaturizer at 0x7fa8bf52e0b0>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_feature_pairs = [\n",
        "    (drug_featurizer(r[\"moleculeSmiles\"]), target_featurizer(r[\"proteinSequence\"]))\n",
        "    for _, r in query_df.iterrows()]\n",
        "\n",
        "dloader = DataLoader(dt_feature_pairs, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f\"Generating predictions...\")\n",
        "preds = []\n",
        "with torch.set_grad_enabled(False):\n",
        "    for b in dloader:\n",
        "        preds.append(model(b[0], b[1]).detach().cpu().numpy())\n",
        "        # print(model(b[0], b[1]).detach().cpu().numpy())\n",
        "\n",
        "# print(preds[0])\n",
        "# print(len(preds[0]))\n",
        "\n",
        "if len(preds[0]) != 1:\n",
        "    preds = np.concatenate(preds)\n",
        "\n",
        "result_df = pd.DataFrame(query_df[[\"moleculeID\", \"proteinID\"]])\n",
        "result_df[\"Prediction\"] = preds\n",
        "\n",
        "# print(f\"Printing ConPLex results to {outfile}\")\n",
        "# result_df.to_csv(outfile, sep=\"\\t\", index=False, header=False)\n",
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "cKg9RwYOOmjl",
        "outputId": "ae559922-c9cd-4083-f5b9-01d2bad15ada"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      moleculeID        proteinID  Prediction\n",
              "0  CHEMBL1287853  ENSP00000371067    0.307619\n",
              "1  CHEMBL1287853  ENSP00000371067    0.250991"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-1a7024a5-9ec5-455d-8e2b-2ca5496b4270\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>moleculeID</th>\n",
              "      <th>proteinID</th>\n",
              "      <th>Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CHEMBL1287853</td>\n",
              "      <td>ENSP00000371067</td>\n",
              "      <td>0.307619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CHEMBL1287853</td>\n",
              "      <td>ENSP00000371067</td>\n",
              "      <td>0.250991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a7024a5-9ec5-455d-8e2b-2ca5496b4270')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-1fec72b3-c0b9-49a7-beea-5116d060b395\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1fec72b3-c0b9-49a7-beea-5116d060b395')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-1fec72b3-c0b9-49a7-beea-5116d060b395 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a7024a5-9ec5-455d-8e2b-2ca5496b4270 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a7024a5-9ec5-455d-8e2b-2ca5496b4270');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vr8-kYKAOspo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}